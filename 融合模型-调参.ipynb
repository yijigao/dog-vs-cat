{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, merge, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/Machine Learning/dogs_vs_cats-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Machine Learning\\\\dogs_vs_cats-master'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立符号链接\n",
    "def data_symbol_link():\n",
    "    work_dir  = os.getcwd()\n",
    "    train_dir = work_dir + \"\\\\train\\\\\"\n",
    "    test_dir  = work_dir + \"\\\\test\\\\\"\n",
    "    data_dir  = work_dir + \"\\\\data\\\\\"\n",
    "    \n",
    "    if(os.path.exists(data_dir)):\n",
    "        shutil.rmtree(data_dir)\n",
    "        \n",
    "    split_train_dir = work_dir+\"\\\\data\\\\train\"\n",
    "    split_test_dir  = work_dir+\"\\\\data\\\\test\"\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "    os.mkdir(split_train_dir)\n",
    "    os.mkdir(split_train_dir+\"\\\\dog\")\n",
    "    os.mkdir(split_train_dir+\"\\\\cat\")\n",
    "    os.mkdir(split_test_dir)\n",
    "    os.mkdir(split_test_dir+\"\\\\test\")\n",
    "        \n",
    "    train_files = os.listdir(train_dir)    \n",
    "    num_train_files = len(train_files)\n",
    "    for i in tqdm(range(num_train_files)):\n",
    "        file = train_files[i]\n",
    "        if \"dog\" in file.split('.'):\n",
    "            os.symlink(train_dir+file, split_train_dir+\"\\\\dog\\\\\"+file)\n",
    "        else:\n",
    "            os.symlink(train_dir+file, split_train_dir+\"\\\\cat\\\\\"+file)\n",
    "    \n",
    "    test_files = os.listdir(test_dir)    \n",
    "    num_test_files = len(test_files)\n",
    "    for i in tqdm(range(num_test_files)):\n",
    "        file = test_files[i]\n",
    "        os.symlink(test_dir+file, split_test_dir+\"\\\\test\\\\\"+file)\n",
    "        \n",
    "    return split_train_dir, split_test_dir    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:06<00:00, 4133.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 4210.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data_symbol_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for filename in [\"../features/feature_xception.h5\", \"../features/feature_densenet201.h5\", \"../features/feature_inception_v3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        Y_train = np.array(h['label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立调参训练函数\n",
    "def train_model(dropout, optimizer):\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    x = Dropout(dropout)(input_tensor)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    filepath=f\"model_weights/{optimizer}_{dropout}.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2, shuffle=True,\n",
    "             callbacks=callbacks_list)\n",
    "    model.save_weights(f\"model_weights/{optimizer}_{dropout}.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_model(dropout, optimizer):\n",
    "    model = train_model(dropout, optimizer)\n",
    "    model.load_weights(f\"model_weights/{optimizer}_{dropout}.h5\")\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    y_test = y_pred.clip(min=0.005, max=0.995)\n",
    "    df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    test_generator = gen.flow_from_directory(test_data, (224, 224), shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "\n",
    "    for i, fname in enumerate(test_generator.filenames):\n",
    "        index = int(fname[fname.rfind('\\\\')+1:fname.rfind('.')])\n",
    "        df.set_value(index-1, 'label', y_test[i])\n",
    "\n",
    "    df.to_csv(f'submission/{optimizer}_{dropout}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 37s - loss: 0.9305 - acc: 0.37 - ETA: 9s - loss: 0.5732 - acc: 0.6937 - ETA: 5s - loss: 0.4361 - acc: 0.802 - ETA: 4s - loss: 0.3535 - acc: 0.854 - ETA: 3s - loss: 0.2998 - acc: 0.884 - ETA: 3s - loss: 0.2612 - acc: 0.905 - ETA: 3s - loss: 0.2332 - acc: 0.918 - ETA: 2s - loss: 0.2130 - acc: 0.927 - ETA: 2s - loss: 0.1955 - acc: 0.934 - ETA: 2s - loss: 0.1823 - acc: 0.940 - ETA: 2s - loss: 0.1690 - acc: 0.945 - ETA: 2s - loss: 0.1587 - acc: 0.949 - ETA: 1s - loss: 0.1494 - acc: 0.952 - ETA: 1s - loss: 0.1411 - acc: 0.955 - ETA: 1s - loss: 0.1342 - acc: 0.958 - ETA: 1s - loss: 0.1278 - acc: 0.960 - ETA: 1s - loss: 0.1231 - acc: 0.962 - ETA: 1s - loss: 0.1177 - acc: 0.964 - ETA: 1s - loss: 0.1130 - acc: 0.966 - ETA: 1s - loss: 0.1087 - acc: 0.967 - ETA: 1s - loss: 0.1055 - acc: 0.968 - ETA: 1s - loss: 0.1018 - acc: 0.970 - ETA: 1s - loss: 0.0992 - acc: 0.970 - ETA: 1s - loss: 0.0965 - acc: 0.971 - ETA: 1s - loss: 0.0935 - acc: 0.972 - ETA: 0s - loss: 0.0916 - acc: 0.973 - ETA: 0s - loss: 0.0897 - acc: 0.973 - ETA: 0s - loss: 0.0874 - acc: 0.974 - ETA: 0s - loss: 0.0858 - acc: 0.975 - ETA: 0s - loss: 0.0839 - acc: 0.975 - ETA: 0s - loss: 0.0819 - acc: 0.976 - ETA: 0s - loss: 0.0804 - acc: 0.976 - ETA: 0s - loss: 0.0795 - acc: 0.977 - ETA: 0s - loss: 0.0782 - acc: 0.977 - ETA: 0s - loss: 0.0772 - acc: 0.977 - ETA: 0s - loss: 0.0758 - acc: 0.978 - ETA: 0s - loss: 0.0748 - acc: 0.978 - ETA: 0s - loss: 0.0738 - acc: 0.978 - ETA: 0s - loss: 0.0725 - acc: 0.979 - ETA: 0s - loss: 0.0716 - acc: 0.979 - ETA: 0s - loss: 0.0707 - acc: 0.979 - ETA: 0s - loss: 0.0698 - acc: 0.980 - ETA: 0s - loss: 0.0687 - acc: 0.980 - ETA: 0s - loss: 0.0682 - acc: 0.980 - 3s 139us/step - loss: 0.0677 - acc: 0.9808 - val_loss: 0.0186 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01859, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0198 - acc: 0.992 - ETA: 2s - loss: 0.0185 - acc: 0.996 - ETA: 2s - loss: 0.0203 - acc: 0.995 - ETA: 2s - loss: 0.0198 - acc: 0.994 - ETA: 2s - loss: 0.0200 - acc: 0.994 - ETA: 2s - loss: 0.0188 - acc: 0.994 - ETA: 2s - loss: 0.0188 - acc: 0.994 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0186 - acc: 0.995 - ETA: 2s - loss: 0.0184 - acc: 0.995 - ETA: 2s - loss: 0.0189 - acc: 0.995 - ETA: 2s - loss: 0.0185 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0180 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.995 - 3s 159us/step - loss: 0.0174 - acc: 0.9953 - val_loss: 0.0137 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01859 to 0.01371, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 3s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0044 - acc: 1.000 - ETA: 2s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0117 - acc: 0.996 - ETA: 2s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0118 - acc: 0.996 - ETA: 2s - loss: 0.0150 - acc: 0.995 - ETA: 2s - loss: 0.0154 - acc: 0.995 - ETA: 2s - loss: 0.0145 - acc: 0.995 - ETA: 2s - loss: 0.0144 - acc: 0.995 - ETA: 2s - loss: 0.0161 - acc: 0.994 - ETA: 2s - loss: 0.0156 - acc: 0.994 - ETA: 2s - loss: 0.0154 - acc: 0.994 - ETA: 2s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - 3s 137us/step - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0137 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01371 to 0.01371, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0463 - acc: 0.984 - ETA: 2s - loss: 0.0183 - acc: 0.993 - ETA: 2s - loss: 0.0142 - acc: 0.995 - ETA: 2s - loss: 0.0132 - acc: 0.995 - ETA: 2s - loss: 0.0109 - acc: 0.996 - ETA: 2s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - 3s 129us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.0130 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01371 to 0.01302, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0100 - acc: 0.992 - ETA: 2s - loss: 0.0151 - acc: 0.996 - ETA: 2s - loss: 0.0107 - acc: 0.997 - ETA: 2s - loss: 0.0125 - acc: 0.996 - ETA: 2s - loss: 0.0127 - acc: 0.996 - ETA: 2s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 2s - loss: 0.0115 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.995 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - 2s 120us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01302 to 0.01151, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0057 - acc: 1.000 - ETA: 2s - loss: 0.0239 - acc: 0.990 - ETA: 2s - loss: 0.0149 - acc: 0.994 - ETA: 2s - loss: 0.0129 - acc: 0.995 - ETA: 2s - loss: 0.0142 - acc: 0.994 - ETA: 2s - loss: 0.0168 - acc: 0.994 - ETA: 2s - loss: 0.0145 - acc: 0.994 - ETA: 2s - loss: 0.0127 - acc: 0.995 - ETA: 2s - loss: 0.0121 - acc: 0.995 - ETA: 2s - loss: 0.0116 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0109 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0110 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - 3s 128us/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0113 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01151 to 0.01135, saving model to model_weights/adadelta_0.5.h5\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0068 - acc: 1.000 - ETA: 2s - loss: 0.0058 - acc: 1.000 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0091 - acc: 0.997 - ETA: 2s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0128 - acc: 0.995 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0102 - acc: 0.996 - ETA: 0s - loss: 0.0102 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0096 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - 2s 123us/step - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0121 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 3.2746e-04 - acc: 1.000 - ETA: 2s - loss: 0.0040 - acc: 1.0000    - ETA: 2s - loss: 0.0031 - acc: 1.000 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0072 - acc: 0.998 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0067 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.996 - ETA: 0s - loss: 0.0094 - acc: 0.996 - ETA: 0s - loss: 0.0092 - acc: 0.996 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - 2s 125us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0119 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0070 - acc: 1.000 - ETA: 2s - loss: 0.0044 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0065 - acc: 0.999 - ETA: 2s - loss: 0.0055 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0046 - acc: 0.999 - ETA: 2s - loss: 0.0051 - acc: 0.999 - ETA: 2s - loss: 0.0050 - acc: 0.999 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - 2s 125us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.0119 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0077 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.999 - ETA: 2s - loss: 0.0088 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - 3s 132us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0122 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 6.3544e-04 - acc: 1.000 - ETA: 2s - loss: 0.0107 - acc: 0.9969    - ETA: 2s - loss: 0.0147 - acc: 0.996 - ETA: 2s - loss: 0.0110 - acc: 0.997 - ETA: 2s - loss: 0.0113 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - 2s 121us/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0119 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0036 - acc: 1.000 - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0028 - acc: 1.000 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - 3s 132us/step - loss: 0.0082 - acc: 0.9972 - val_loss: 0.0120 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0022 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0042 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - 2s 120us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0142 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 4.7102e-04 - acc: 1.000 - ETA: 2s - loss: 0.0018 - acc: 1.0000    - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0042 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - 3s 126us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0120 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 1.000 - ETA: 2s - loss: 0.0022 - acc: 0.999 - ETA: 2s - loss: 0.0027 - acc: 0.999 - ETA: 2s - loss: 0.0028 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.997 - ETA: 1s - loss: 0.0067 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.997 - 3s 126us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0137 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0076 - acc: 0.996 - ETA: 1s - loss: 0.0082 - acc: 0.996 - ETA: 1s - loss: 0.0085 - acc: 0.995 - ETA: 1s - loss: 0.0085 - acc: 0.995 - ETA: 1s - loss: 0.0106 - acc: 0.995 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0102 - acc: 0.996 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0094 - acc: 0.996 - ETA: 1s - loss: 0.0090 - acc: 0.996 - ETA: 1s - loss: 0.0089 - acc: 0.996 - ETA: 1s - loss: 0.0085 - acc: 0.996 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - 2s 125us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0120 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0129 - acc: 0.992 - ETA: 2s - loss: 0.0116 - acc: 0.996 - ETA: 2s - loss: 0.0077 - acc: 0.997 - ETA: 2s - loss: 0.0071 - acc: 0.997 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0056 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0072 - acc: 0.997 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 3s 127us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0123 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 6.2903e-04 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 1.0000    - ETA: 2s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 2s - loss: 0.0025 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0055 - acc: 0.997 - ETA: 1s - loss: 0.0054 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 2s 121us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0123 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0100 - acc: 0.992 - ETA: 2s - loss: 0.0040 - acc: 0.996 - ETA: 2s - loss: 0.0108 - acc: 0.994 - ETA: 2s - loss: 0.0079 - acc: 0.996 - ETA: 2s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.996 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.996 - ETA: 1s - loss: 0.0069 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.996 - ETA: 1s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0076 - acc: 0.996 - ETA: 1s - loss: 0.0074 - acc: 0.996 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0057 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - 2s 123us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0135 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0018 - acc: 1.000 - ETA: 2s - loss: 0.0031 - acc: 1.000 - ETA: 2s - loss: 0.0060 - acc: 0.999 - ETA: 2s - loss: 0.0058 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0048 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0054 - acc: 0.999 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 3s 131us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0127 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 17 - ETA: 0 - ETA:  - ETA:  - ETA:  - ETA:  - 0s 25us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(0.5, 'adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 43s - loss: 0.8161 - acc: 0.48 - ETA: 10s - loss: 0.5671 - acc: 0.70 - ETA: 6s - loss: 0.4152 - acc: 0.8142 - ETA: 5s - loss: 0.3197 - acc: 0.865 - ETA: 4s - loss: 0.2661 - acc: 0.892 - ETA: 3s - loss: 0.2287 - acc: 0.910 - ETA: 3s - loss: 0.1987 - acc: 0.924 - ETA: 2s - loss: 0.1759 - acc: 0.934 - ETA: 2s - loss: 0.1637 - acc: 0.939 - ETA: 2s - loss: 0.1529 - acc: 0.943 - ETA: 2s - loss: 0.1436 - acc: 0.947 - ETA: 2s - loss: 0.1344 - acc: 0.951 - ETA: 2s - loss: 0.1297 - acc: 0.953 - ETA: 2s - loss: 0.1214 - acc: 0.956 - ETA: 2s - loss: 0.1148 - acc: 0.959 - ETA: 2s - loss: 0.1086 - acc: 0.962 - ETA: 1s - loss: 0.1035 - acc: 0.964 - ETA: 1s - loss: 0.0989 - acc: 0.966 - ETA: 1s - loss: 0.0962 - acc: 0.967 - ETA: 1s - loss: 0.0922 - acc: 0.968 - ETA: 1s - loss: 0.0885 - acc: 0.970 - ETA: 1s - loss: 0.0857 - acc: 0.971 - ETA: 1s - loss: 0.0831 - acc: 0.972 - ETA: 1s - loss: 0.0800 - acc: 0.973 - ETA: 1s - loss: 0.0775 - acc: 0.974 - ETA: 1s - loss: 0.0755 - acc: 0.975 - ETA: 1s - loss: 0.0731 - acc: 0.975 - ETA: 0s - loss: 0.0714 - acc: 0.976 - ETA: 0s - loss: 0.0693 - acc: 0.977 - ETA: 0s - loss: 0.0676 - acc: 0.978 - ETA: 0s - loss: 0.0663 - acc: 0.978 - ETA: 0s - loss: 0.0645 - acc: 0.979 - ETA: 0s - loss: 0.0635 - acc: 0.979 - ETA: 0s - loss: 0.0624 - acc: 0.979 - ETA: 0s - loss: 0.0610 - acc: 0.980 - ETA: 0s - loss: 0.0602 - acc: 0.980 - ETA: 0s - loss: 0.0588 - acc: 0.981 - ETA: 0s - loss: 0.0585 - acc: 0.980 - ETA: 0s - loss: 0.0576 - acc: 0.981 - ETA: 0s - loss: 0.0565 - acc: 0.981 - ETA: 0s - loss: 0.0554 - acc: 0.982 - ETA: 0s - loss: 0.0544 - acc: 0.982 - 3s 141us/step - loss: 0.0538 - acc: 0.9826 - val_loss: 0.0161 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01606, saving model to model_weights/adam_0.5.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0095 - acc: 1.000 - ETA: 2s - loss: 0.0201 - acc: 0.993 - ETA: 1s - loss: 0.0202 - acc: 0.993 - ETA: 1s - loss: 0.0209 - acc: 0.993 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.993 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - 2s 109us/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0130 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01606 to 0.01300, saving model to model_weights/adam_0.5.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0045 - acc: 1.000 - ETA: 1s - loss: 0.0122 - acc: 0.997 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - 2s 111us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0116 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01300 to 0.01163, saving model to model_weights/adam_0.5.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0095 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.995 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.995 - ETA: 1s - loss: 0.0110 - acc: 0.995 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.995 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0124 - acc: 0.995 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - 2s 123us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01163 to 0.01152, saving model to model_weights/adam_0.5.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0102 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.995 - ETA: 1s - loss: 0.0119 - acc: 0.995 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.996 - ETA: 0s - loss: 0.0110 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.996 - 2s 108us/step - loss: 0.0112 - acc: 0.9962 - val_loss: 0.0114 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01152 to 0.01138, saving model to model_weights/adam_0.5.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0209 - acc: 0.992 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0100 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.998 - ETA: 1s - loss: 0.0083 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0109 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0101 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.996 - ETA: 0s - loss: 0.0105 - acc: 0.996 - ETA: 0s - loss: 0.0106 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.996 - ETA: 0s - loss: 0.0102 - acc: 0.996 - ETA: 0s - loss: 0.0101 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.996 - 2s 104us/step - loss: 0.0104 - acc: 0.9966 - val_loss: 0.0131 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0047 - acc: 1.000 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0091 - acc: 0.998 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0102 - acc: 0.996 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.996 - ETA: 1s - loss: 0.0096 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.996 - ETA: 1s - loss: 0.0092 - acc: 0.996 - ETA: 1s - loss: 0.0089 - acc: 0.996 - ETA: 1s - loss: 0.0088 - acc: 0.996 - ETA: 0s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0090 - acc: 0.996 - ETA: 0s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0091 - acc: 0.996 - ETA: 0s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0092 - acc: 0.996 - ETA: 0s - loss: 0.0091 - acc: 0.996 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0096 - acc: 0.996 - ETA: 0s - loss: 0.0094 - acc: 0.996 - ETA: 0s - loss: 0.0098 - acc: 0.996 - ETA: 0s - loss: 0.0097 - acc: 0.996 - ETA: 0s - loss: 0.0095 - acc: 0.996 - ETA: 0s - loss: 0.0096 - acc: 0.996 - ETA: 0s - loss: 0.0094 - acc: 0.996 - 2s 107us/step - loss: 0.0096 - acc: 0.9966 - val_loss: 0.0118 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0136 - acc: 0.992 - ETA: 1s - loss: 0.0126 - acc: 0.994 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 1s - loss: 0.0094 - acc: 0.996 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - 2s 113us/step - loss: 0.0093 - acc: 0.9971 - val_loss: 0.0116 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0023 - acc: 1.000 - ETA: 2s - loss: 0.0036 - acc: 1.000 - ETA: 2s - loss: 0.0093 - acc: 0.996 - ETA: 2s - loss: 0.0068 - acc: 0.997 - ETA: 2s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - 2s 120us/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0127 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 9.6416e-04 - acc: 1.000 - ETA: 1s - loss: 0.0061 - acc: 0.9987    - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.997 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - 2s 111us/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0123 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - 3s 132us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0136 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0099 - acc: 0.992 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - 2s 104us/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0125 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0043 - acc: 1.000 - ETA: 2s - loss: 0.0055 - acc: 0.996 - ETA: 1s - loss: 0.0058 - acc: 0.996 - ETA: 1s - loss: 0.0050 - acc: 0.997 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0069 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - 2s 105us/step - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0151 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 1s - loss: 0.0074 - acc: 0.996 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - 2s 106us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0129 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0026 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - 2s 105us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0125 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0090 - acc: 1.000 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - ETA: 0s - loss: 0.0065 - acc: 0.997 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.997 - 2s 103us/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0127 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 1s - loss: 0.0035 - acc: 1.000 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - 2s 99us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0132 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0049 - acc: 0.997 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - 2s 104us/step - loss: 0.0050 - acc: 0.9980 - val_loss: 0.0133 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - 2s 102us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0131 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 5.1809e-04 - acc: 1.000 - ETA: 1s - loss: 0.0070 - acc: 0.9974    - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - 2s 104us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0134 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 19 - ETA: 0 - ETA:  - ETA:  - 0s 18us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(0.5, 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 42s - loss: 0.7871 - acc: 0.51 - ETA: 10s - loss: 0.5377 - acc: 0.73 - ETA: 6s - loss: 0.4222 - acc: 0.8273 - ETA: 4s - loss: 0.3517 - acc: 0.871 - ETA: 4s - loss: 0.3052 - acc: 0.897 - ETA: 3s - loss: 0.2728 - acc: 0.911 - ETA: 3s - loss: 0.2473 - acc: 0.922 - ETA: 3s - loss: 0.2335 - acc: 0.929 - ETA: 2s - loss: 0.2181 - acc: 0.934 - ETA: 2s - loss: 0.2039 - acc: 0.939 - ETA: 2s - loss: 0.1931 - acc: 0.943 - ETA: 2s - loss: 0.1831 - acc: 0.947 - ETA: 2s - loss: 0.1732 - acc: 0.951 - ETA: 2s - loss: 0.1651 - acc: 0.953 - ETA: 1s - loss: 0.1573 - acc: 0.956 - ETA: 1s - loss: 0.1510 - acc: 0.958 - ETA: 1s - loss: 0.1457 - acc: 0.960 - ETA: 1s - loss: 0.1405 - acc: 0.962 - ETA: 1s - loss: 0.1356 - acc: 0.963 - ETA: 1s - loss: 0.1317 - acc: 0.965 - ETA: 1s - loss: 0.1276 - acc: 0.966 - ETA: 1s - loss: 0.1253 - acc: 0.967 - ETA: 1s - loss: 0.1218 - acc: 0.968 - ETA: 1s - loss: 0.1182 - acc: 0.969 - ETA: 1s - loss: 0.1151 - acc: 0.970 - ETA: 1s - loss: 0.1136 - acc: 0.970 - ETA: 0s - loss: 0.1118 - acc: 0.970 - ETA: 0s - loss: 0.1100 - acc: 0.971 - ETA: 0s - loss: 0.1080 - acc: 0.972 - ETA: 0s - loss: 0.1057 - acc: 0.972 - ETA: 0s - loss: 0.1036 - acc: 0.973 - ETA: 0s - loss: 0.1018 - acc: 0.973 - ETA: 0s - loss: 0.0995 - acc: 0.974 - ETA: 0s - loss: 0.0972 - acc: 0.975 - ETA: 0s - loss: 0.0960 - acc: 0.975 - ETA: 0s - loss: 0.0948 - acc: 0.975 - ETA: 0s - loss: 0.0933 - acc: 0.976 - ETA: 0s - loss: 0.0920 - acc: 0.976 - ETA: 0s - loss: 0.0903 - acc: 0.977 - ETA: 0s - loss: 0.0891 - acc: 0.977 - 3s 133us/step - loss: 0.0891 - acc: 0.9774 - val_loss: 0.0316 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03156, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0312 - acc: 1.000 - ETA: 1s - loss: 0.0410 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.992 - ETA: 1s - loss: 0.0355 - acc: 0.994 - ETA: 1s - loss: 0.0327 - acc: 0.994 - ETA: 1s - loss: 0.0348 - acc: 0.993 - ETA: 1s - loss: 0.0365 - acc: 0.992 - ETA: 1s - loss: 0.0373 - acc: 0.991 - ETA: 1s - loss: 0.0380 - acc: 0.992 - ETA: 1s - loss: 0.0378 - acc: 0.992 - ETA: 1s - loss: 0.0388 - acc: 0.991 - ETA: 1s - loss: 0.0384 - acc: 0.991 - ETA: 1s - loss: 0.0378 - acc: 0.991 - ETA: 1s - loss: 0.0378 - acc: 0.992 - ETA: 1s - loss: 0.0374 - acc: 0.992 - ETA: 1s - loss: 0.0369 - acc: 0.992 - ETA: 1s - loss: 0.0364 - acc: 0.992 - ETA: 1s - loss: 0.0361 - acc: 0.992 - ETA: 1s - loss: 0.0361 - acc: 0.992 - ETA: 1s - loss: 0.0356 - acc: 0.992 - ETA: 0s - loss: 0.0360 - acc: 0.992 - ETA: 0s - loss: 0.0361 - acc: 0.992 - ETA: 0s - loss: 0.0356 - acc: 0.992 - ETA: 0s - loss: 0.0361 - acc: 0.992 - ETA: 0s - loss: 0.0356 - acc: 0.992 - ETA: 0s - loss: 0.0351 - acc: 0.992 - ETA: 0s - loss: 0.0347 - acc: 0.992 - ETA: 0s - loss: 0.0351 - acc: 0.992 - ETA: 0s - loss: 0.0350 - acc: 0.992 - ETA: 0s - loss: 0.0353 - acc: 0.992 - ETA: 0s - loss: 0.0353 - acc: 0.992 - ETA: 0s - loss: 0.0356 - acc: 0.992 - ETA: 0s - loss: 0.0354 - acc: 0.992 - ETA: 0s - loss: 0.0348 - acc: 0.992 - ETA: 0s - loss: 0.0352 - acc: 0.992 - ETA: 0s - loss: 0.0350 - acc: 0.992 - ETA: 0s - loss: 0.0346 - acc: 0.992 - 2s 107us/step - loss: 0.0345 - acc: 0.9926 - val_loss: 0.0229 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03156 to 0.02285, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0168 - acc: 1.000 - ETA: 1s - loss: 0.0309 - acc: 0.990 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 1s - loss: 0.0275 - acc: 0.992 - ETA: 1s - loss: 0.0274 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0277 - acc: 0.992 - ETA: 1s - loss: 0.0282 - acc: 0.992 - ETA: 1s - loss: 0.0272 - acc: 0.992 - ETA: 1s - loss: 0.0270 - acc: 0.992 - ETA: 1s - loss: 0.0272 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0281 - acc: 0.992 - ETA: 0s - loss: 0.0277 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0290 - acc: 0.992 - ETA: 0s - loss: 0.0292 - acc: 0.992 - ETA: 0s - loss: 0.0294 - acc: 0.992 - ETA: 0s - loss: 0.0291 - acc: 0.992 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0284 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.992 - ETA: 0s - loss: 0.0283 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0283 - acc: 0.992 - ETA: 0s - loss: 0.0281 - acc: 0.992 - 2s 104us/step - loss: 0.0283 - acc: 0.9923 - val_loss: 0.0197 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02285 to 0.01966, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0221 - acc: 1.000 - ETA: 2s - loss: 0.0192 - acc: 0.996 - ETA: 1s - loss: 0.0250 - acc: 0.994 - ETA: 1s - loss: 0.0222 - acc: 0.996 - ETA: 1s - loss: 0.0224 - acc: 0.995 - ETA: 2s - loss: 0.0224 - acc: 0.996 - ETA: 1s - loss: 0.0230 - acc: 0.996 - ETA: 1s - loss: 0.0223 - acc: 0.996 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0238 - acc: 0.994 - ETA: 1s - loss: 0.0245 - acc: 0.994 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0258 - acc: 0.993 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0255 - acc: 0.993 - ETA: 1s - loss: 0.0246 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0238 - acc: 0.993 - ETA: 1s - loss: 0.0241 - acc: 0.993 - ETA: 1s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0232 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0244 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0250 - acc: 0.993 - ETA: 0s - loss: 0.0253 - acc: 0.993 - ETA: 0s - loss: 0.0252 - acc: 0.993 - ETA: 0s - loss: 0.0251 - acc: 0.993 - 2s 108us/step - loss: 0.0251 - acc: 0.9932 - val_loss: 0.0184 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01966 to 0.01837, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0274 - acc: 0.984 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0217 - acc: 0.992 - ETA: 1s - loss: 0.0190 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0272 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0239 - acc: 0.993 - ETA: 1s - loss: 0.0250 - acc: 0.992 - ETA: 1s - loss: 0.0250 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 1s - loss: 0.0241 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0238 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0218 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0222 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0226 - acc: 0.994 - 2s 99us/step - loss: 0.0226 - acc: 0.9940 - val_loss: 0.0167 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01837 to 0.01669, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0333 - acc: 0.992 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.994 - ETA: 1s - loss: 0.0216 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0221 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0216 - acc: 0.995 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0216 - acc: 0.995 - ETA: 1s - loss: 0.0208 - acc: 0.995 - ETA: 1s - loss: 0.0207 - acc: 0.995 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0217 - acc: 0.994 - ETA: 1s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0224 - acc: 0.994 - ETA: 0s - loss: 0.0224 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0226 - acc: 0.994 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - 2s 107us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0160 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01669 to 0.01597, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0110 - acc: 1.000 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.995 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0214 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0210 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - 2s 105us/step - loss: 0.0215 - acc: 0.9939 - val_loss: 0.0155 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01597 to 0.01549, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0159 - acc: 0.992 - ETA: 1s - loss: 0.0327 - acc: 0.989 - ETA: 1s - loss: 0.0287 - acc: 0.990 - ETA: 1s - loss: 0.0285 - acc: 0.989 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0228 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 1s - loss: 0.0224 - acc: 0.992 - ETA: 1s - loss: 0.0226 - acc: 0.993 - ETA: 1s - loss: 0.0222 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0208 - acc: 0.994 - ETA: 0s - loss: 0.0206 - acc: 0.994 - 2s 101us/step - loss: 0.0208 - acc: 0.9940 - val_loss: 0.0150 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01549 to 0.01505, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0077 - acc: 1.000 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.992 - ETA: 1s - loss: 0.0221 - acc: 0.992 - ETA: 1s - loss: 0.0205 - acc: 0.992 - ETA: 1s - loss: 0.0201 - acc: 0.992 - ETA: 1s - loss: 0.0193 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0201 - acc: 0.993 - ETA: 1s - loss: 0.0202 - acc: 0.993 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0206 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0200 - acc: 0.993 - ETA: 0s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - 2s 104us/step - loss: 0.0206 - acc: 0.9938 - val_loss: 0.0148 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01505 to 0.01482, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0156 - acc: 0.992 - ETA: 2s - loss: 0.0209 - acc: 0.990 - ETA: 2s - loss: 0.0190 - acc: 0.991 - ETA: 1s - loss: 0.0171 - acc: 0.992 - ETA: 1s - loss: 0.0189 - acc: 0.992 - ETA: 1s - loss: 0.0177 - acc: 0.993 - ETA: 1s - loss: 0.0175 - acc: 0.993 - ETA: 1s - loss: 0.0174 - acc: 0.993 - ETA: 1s - loss: 0.0176 - acc: 0.993 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.993 - ETA: 1s - loss: 0.0181 - acc: 0.993 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0181 - acc: 0.993 - ETA: 1s - loss: 0.0199 - acc: 0.993 - ETA: 1s - loss: 0.0201 - acc: 0.993 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0195 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0200 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0198 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0196 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0194 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.993 - ETA: 0s - loss: 0.0192 - acc: 0.994 - 2s 107us/step - loss: 0.0192 - acc: 0.9940 - val_loss: 0.0144 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01482 to 0.01443, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0685 - acc: 0.984 - ETA: 2s - loss: 0.0260 - acc: 0.992 - ETA: 2s - loss: 0.0293 - acc: 0.992 - ETA: 2s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0230 - acc: 0.992 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0204 - acc: 0.993 - ETA: 1s - loss: 0.0195 - acc: 0.993 - ETA: 1s - loss: 0.0195 - acc: 0.993 - ETA: 1s - loss: 0.0193 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - 2s 106us/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0141 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01443 to 0.01406, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0447 - acc: 0.984 - ETA: 2s - loss: 0.0176 - acc: 0.993 - ETA: 1s - loss: 0.0225 - acc: 0.992 - ETA: 1s - loss: 0.0209 - acc: 0.992 - ETA: 1s - loss: 0.0202 - acc: 0.992 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 1s - loss: 0.0179 - acc: 0.993 - ETA: 1s - loss: 0.0188 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.995 - 2s 110us/step - loss: 0.0172 - acc: 0.9949 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01406 to 0.01390, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0099 - acc: 1.000 - ETA: 2s - loss: 0.0174 - acc: 0.992 - ETA: 2s - loss: 0.0177 - acc: 0.992 - ETA: 2s - loss: 0.0142 - acc: 0.994 - ETA: 2s - loss: 0.0157 - acc: 0.994 - ETA: 2s - loss: 0.0158 - acc: 0.994 - ETA: 2s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0164 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - 2s 122us/step - loss: 0.0175 - acc: 0.9950 - val_loss: 0.0139 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01390 to 0.01389, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0252 - acc: 0.992 - ETA: 2s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0102 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.997 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0171 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0154 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - 2s 122us/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01389 to 0.01372, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0520 - acc: 0.992 - ETA: 2s - loss: 0.0183 - acc: 0.996 - ETA: 2s - loss: 0.0237 - acc: 0.993 - ETA: 2s - loss: 0.0227 - acc: 0.994 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0188 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 2s - loss: 0.0188 - acc: 0.995 - ETA: 2s - loss: 0.0185 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - 2s 117us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0288 - acc: 0.992 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - 2s 108us/step - loss: 0.0158 - acc: 0.9955 - val_loss: 0.0135 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01372 to 0.01346, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0200 - acc: 0.992 - ETA: 2s - loss: 0.0090 - acc: 0.996 - ETA: 1s - loss: 0.0174 - acc: 0.993 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0165 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - 2s 102us/step - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0137 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0429 - acc: 0.984 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - 2s 98us/step - loss: 0.0157 - acc: 0.9955 - val_loss: 0.0139 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0084 - acc: 1.000 - ETA: 1s - loss: 0.0090 - acc: 0.998 - ETA: 1s - loss: 0.0103 - acc: 0.998 - ETA: 1s - loss: 0.0170 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.996 - ETA: 1s - loss: 0.0178 - acc: 0.996 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - 2s 109us/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.0133 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01346 to 0.01334, saving model to model_weights/sgd_0.5.h5\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0255 - acc: 0.992 - ETA: 2s - loss: 0.0169 - acc: 0.992 - ETA: 2s - loss: 0.0199 - acc: 0.993 - ETA: 2s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - 2s 115us/step - loss: 0.0160 - acc: 0.9955 - val_loss: 0.0132 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01334 to 0.01320, saving model to model_weights/sgd_0.5.h5\n",
      "12500/12500 [==============================] - ETA: 21 - ETA: 0 - ETA:  - ETA:  - 0s 20us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(0.5, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 42s - loss: 0.5929 - acc: 0.71 - ETA: 2s - loss: 0.2240 - acc: 0.9385 - ETA: 1s - loss: 0.1515 - acc: 0.963 - ETA: 1s - loss: 0.1172 - acc: 0.972 - ETA: 0s - loss: 0.0971 - acc: 0.978 - ETA: 0s - loss: 0.0842 - acc: 0.981 - ETA: 0s - loss: 0.0752 - acc: 0.983 - ETA: 0s - loss: 0.0687 - acc: 0.985 - ETA: 0s - loss: 0.0621 - acc: 0.986 - ETA: 0s - loss: 0.0583 - acc: 0.986 - ETA: 0s - loss: 0.0539 - acc: 0.988 - 1s 46us/step - loss: 0.0528 - acc: 0.9883 - val_loss: 0.0161 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01610, saving model to model_weights/adadelta_1.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0117 - acc: 0.992 - ETA: 0s - loss: 0.0189 - acc: 0.993 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - 1s 30us/step - loss: 0.0154 - acc: 0.9954 - val_loss: 0.0133 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01610 to 0.01335, saving model to model_weights/adadelta_1.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - 1s 30us/step - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0118 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01335 to 0.01184, saving model to model_weights/adadelta_1.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0387 - acc: 0.992 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0106 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.996 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - 1s 30us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0166 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - 1s 33us/step - loss: 0.0080 - acc: 0.9977 - val_loss: 0.0136 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0037 - acc: 1.000 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.998 - 1s 33us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0132 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0088 - acc: 1.000 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 1s 32us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0127 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0037 - acc: 1.000 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - 1s 35us/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0139 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0054 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - 1s 32us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0127 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0038 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - 1s 33us/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0130 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0116 - acc: 0.992 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.998 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - 1s 32us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0132 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 3.5151e-04 - acc: 1.000 - ETA: 0s - loss: 0.0024 - acc: 0.9995    - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - 1s 30us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0137 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0027 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - 1s 29us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0154 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0054 - acc: 1.000 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - 1s 28us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0140 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 9.2963e-04 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.0000    - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - 1s 29us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0150 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - 1s 30us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0148 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 1.7976e-04 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.0000    - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - 1s 30us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0169 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 0.999 - ETA: 0s - loss: 0.0014 - acc: 0.999 - ETA: 0s - loss: 0.0014 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - 1s 28us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0148 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 3.4835e-04 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.0000    - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 0.999 - ETA: 0s - loss: 0.0013 - acc: 0.999 - ETA: 0s - loss: 0.0014 - acc: 0.999 - ETA: 0s - loss: 0.0014 - acc: 0.999 - 1s 28us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0150 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 4.3457e-04 - acc: 1.000 - ETA: 0s - loss: 8.4949e-04 - acc: 1.000 - ETA: 0s - loss: 9.4749e-04 - acc: 1.000 - ETA: 0s - loss: 9.0896e-04 - acc: 1.000 - ETA: 0s - loss: 9.4017e-04 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.0000    - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - 1s 28us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0163 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - 0s 18us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(1, 'adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 46s - loss: 0.6915 - acc: 0.57 - ETA: 3s - loss: 0.2267 - acc: 0.9396 - ETA: 1s - loss: 0.1264 - acc: 0.967 - ETA: 1s - loss: 0.0921 - acc: 0.976 - ETA: 0s - loss: 0.0764 - acc: 0.979 - ETA: 0s - loss: 0.0644 - acc: 0.983 - ETA: 0s - loss: 0.0573 - acc: 0.984 - ETA: 0s - loss: 0.0524 - acc: 0.985 - ETA: 0s - loss: 0.0480 - acc: 0.986 - ETA: 0s - loss: 0.0448 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.988 - 1s 50us/step - loss: 0.0409 - acc: 0.9887 - val_loss: 0.0147 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01472, saving model to model_weights/adam_1.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0074 - acc: 1.000 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.994 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - 1s 30us/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0133 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01472 to 0.01326, saving model to model_weights/adam_1.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0056 - acc: 1.000 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - 1s 31us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0125 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01326 to 0.01251, saving model to model_weights/adam_1.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.992 - ETA: 0s - loss: 0.0057 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - 1s 32us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0132 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0019 - acc: 1.000 - ETA: 0s - loss: 0.0066 - acc: 0.996 - ETA: 0s - loss: 0.0077 - acc: 0.996 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - 1s 32us/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.0154 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0055 - acc: 1.000 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - 1s 32us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 0.0148 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0163 - acc: 0.992 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.999 - ETA: 0s - loss: 0.0050 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.999 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - 1s 32us/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0154 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.999 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - 1s 31us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0132 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0057 - acc: 1.000 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - 1s 33us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0134 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0029 - acc: 1.000 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - 1s 32us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0149 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - 1s 32us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.0157 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0071 - acc: 1.000 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - 1s 34us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0147 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 5.1072e-04 - acc: 1.000 - ETA: 0s - loss: 0.0023 - acc: 0.9995    - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - 1s 31us/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0150 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0044 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - 1s 30us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.0154 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0055 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - 1s 30us/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0151 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 1.7098e-04 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.0000    - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - 1s 30us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 1.5202e-04 - acc: 1.000 - ETA: 0s - loss: 7.7161e-04 - acc: 1.000 - ETA: 0s - loss: 8.5377e-04 - acc: 1.000 - ETA: 0s - loss: 8.6684e-04 - acc: 1.000 - ETA: 0s - loss: 9.9341e-04 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.0000    - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - 1s 30us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 8.7347e-04 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.0000    - ETA: 0s - loss: 9.5507e-04 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.0000    - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - 1s 31us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0158 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 2.4402e-04 - acc: 1.000 - ETA: 0s - loss: 4.9624e-04 - acc: 1.000 - ETA: 0s - loss: 7.1183e-04 - acc: 1.000 - ETA: 0s - loss: 8.1411e-04 - acc: 1.000 - ETA: 0s - loss: 8.6907e-04 - acc: 0.999 - ETA: 0s - loss: 9.1347e-04 - acc: 0.999 - ETA: 0s - loss: 9.4208e-04 - acc: 0.999 - ETA: 0s - loss: 9.0957e-04 - acc: 0.999 - ETA: 0s - loss: 9.2958e-04 - acc: 0.999 - ETA: 0s - loss: 9.3540e-04 - acc: 0.999 - ETA: 0s - loss: 9.2837e-04 - acc: 0.999 - 1s 31us/step - loss: 9.2866e-04 - acc: 0.9999 - val_loss: 0.0161 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 2.9703e-04 - acc: 1.000 - ETA: 0s - loss: 5.3076e-04 - acc: 1.000 - ETA: 0s - loss: 5.7274e-04 - acc: 1.000 - ETA: 0s - loss: 6.9022e-04 - acc: 1.000 - ETA: 0s - loss: 6.6066e-04 - acc: 1.000 - ETA: 0s - loss: 6.7389e-04 - acc: 1.000 - ETA: 0s - loss: 7.2224e-04 - acc: 1.000 - ETA: 0s - loss: 8.0085e-04 - acc: 1.000 - ETA: 0s - loss: 8.0667e-04 - acc: 1.000 - ETA: 0s - loss: 8.0709e-04 - acc: 1.000 - 1s 30us/step - loss: 8.0803e-04 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 25 - ETA: 0 - ETA:  - ETA:  - 0s 20us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(1, 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 44s - loss: 0.7244 - acc: 0.55 - ETA: 3s - loss: 0.2907 - acc: 0.9224 - ETA: 1s - loss: 0.1942 - acc: 0.956 - ETA: 1s - loss: 0.1541 - acc: 0.967 - ETA: 0s - loss: 0.1308 - acc: 0.973 - ETA: 0s - loss: 0.1141 - acc: 0.977 - ETA: 0s - loss: 0.1024 - acc: 0.979 - ETA: 0s - loss: 0.0952 - acc: 0.981 - ETA: 0s - loss: 0.0882 - acc: 0.982 - ETA: 0s - loss: 0.0824 - acc: 0.983 - ETA: 0s - loss: 0.0778 - acc: 0.984 - 1s 47us/step - loss: 0.0775 - acc: 0.9843 - val_loss: 0.0345 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03451, saving model to model_weights/sgd_1.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0141 - acc: 1.000 - ETA: 0s - loss: 0.0319 - acc: 0.993 - ETA: 0s - loss: 0.0352 - acc: 0.991 - ETA: 0s - loss: 0.0339 - acc: 0.992 - ETA: 0s - loss: 0.0326 - acc: 0.992 - ETA: 0s - loss: 0.0327 - acc: 0.992 - ETA: 0s - loss: 0.0328 - acc: 0.992 - ETA: 0s - loss: 0.0323 - acc: 0.992 - ETA: 0s - loss: 0.0311 - acc: 0.992 - ETA: 0s - loss: 0.0303 - acc: 0.993 - ETA: 0s - loss: 0.0301 - acc: 0.993 - 1s 32us/step - loss: 0.0299 - acc: 0.9931 - val_loss: 0.0255 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03451 to 0.02550, saving model to model_weights/sgd_1.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0498 - acc: 0.984 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0270 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.993 - ETA: 0s - loss: 0.0246 - acc: 0.993 - ETA: 0s - loss: 0.0247 - acc: 0.993 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0242 - acc: 0.993 - 1s 30us/step - loss: 0.0243 - acc: 0.9936 - val_loss: 0.0221 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02550 to 0.02214, saving model to model_weights/sgd_1.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0414 - acc: 0.984 - ETA: 0s - loss: 0.0234 - acc: 0.994 - ETA: 0s - loss: 0.0265 - acc: 0.994 - ETA: 0s - loss: 0.0238 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.995 - ETA: 0s - loss: 0.0218 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - 1s 29us/step - loss: 0.0216 - acc: 0.9941 - val_loss: 0.0200 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02214 to 0.02002, saving model to model_weights/sgd_1.h5\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - 1s 29us/step - loss: 0.0197 - acc: 0.9946 - val_loss: 0.0188 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02002 to 0.01881, saving model to model_weights/sgd_1.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0357 - acc: 0.984 - ETA: 0s - loss: 0.0194 - acc: 0.993 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0186 - acc: 0.994 - 1s 29us/step - loss: 0.0185 - acc: 0.9948 - val_loss: 0.0179 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01881 to 0.01786, saving model to model_weights/sgd_1.h5\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0378 - acc: 0.992 - ETA: 0s - loss: 0.0177 - acc: 0.993 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - 1s 28us/step - loss: 0.0175 - acc: 0.9949 - val_loss: 0.0172 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01786 to 0.01725, saving model to model_weights/sgd_1.h5\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0047 - acc: 1.000 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0131 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - 1s 30us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 0.0167 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01725 to 0.01671, saving model to model_weights/sgd_1.h5\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0164 - acc: 1.000 - ETA: 0s - loss: 0.0172 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - 1s 28us/step - loss: 0.0161 - acc: 0.9953 - val_loss: 0.0163 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01671 to 0.01629, saving model to model_weights/sgd_1.h5\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - 1s 28us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.0160 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01629 to 0.01595, saving model to model_weights/sgd_1.h5\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0102 - acc: 1.000 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - 1s 29us/step - loss: 0.0151 - acc: 0.9957 - val_loss: 0.0157 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01595 to 0.01568, saving model to model_weights/sgd_1.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0069 - acc: 1.000 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - 1s 30us/step - loss: 0.0147 - acc: 0.9958 - val_loss: 0.0155 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01568 to 0.01546, saving model to model_weights/sgd_1.h5\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0203 - acc: 0.992 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - 1s 28us/step - loss: 0.0143 - acc: 0.9961 - val_loss: 0.0152 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01546 to 0.01524, saving model to model_weights/sgd_1.h5\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0091 - acc: 1.000 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - 1s 29us/step - loss: 0.0139 - acc: 0.9962 - val_loss: 0.0152 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01524 to 0.01516, saving model to model_weights/sgd_1.h5\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0089 - acc: 1.000 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - 1s 29us/step - loss: 0.0137 - acc: 0.9962 - val_loss: 0.0150 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01516 to 0.01498, saving model to model_weights/sgd_1.h5\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.992 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - 1s 28us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.0148 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01498 to 0.01481, saving model to model_weights/sgd_1.h5\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0174 - acc: 0.992 - ETA: 0s - loss: 0.0118 - acc: 0.995 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - 1s 30us/step - loss: 0.0131 - acc: 0.9964 - val_loss: 0.0148 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.992 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.996 - 1s 29us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0146 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01481 to 0.01460, saving model to model_weights/sgd_1.h5\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0154 - acc: 1.000 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0108 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - 1s 28us/step - loss: 0.0126 - acc: 0.9964 - val_loss: 0.0146 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01460 to 0.01459, saving model to model_weights/sgd_1.h5\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 0s - loss: 0.0089 - acc: 1.000 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0108 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - 1s 30us/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0145 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01459 to 0.01447, saving model to model_weights/sgd_1.h5\n",
      "12500/12500 [==============================] - ETA: 26 - ETA: 0 - ETA:  - ETA:  - 0s 19us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_predict_model(1, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 51s - loss: 0.8159 - acc: 0.39 - ETA: 11s - loss: 0.5029 - acc: 0.74 - ETA: 7s - loss: 0.3727 - acc: 0.8446 - ETA: 5s - loss: 0.2964 - acc: 0.888 - ETA: 4s - loss: 0.2534 - acc: 0.910 - ETA: 3s - loss: 0.2263 - acc: 0.922 - ETA: 3s - loss: 0.2031 - acc: 0.933 - ETA: 3s - loss: 0.1844 - acc: 0.941 - ETA: 2s - loss: 0.1696 - acc: 0.947 - ETA: 2s - loss: 0.1564 - acc: 0.953 - ETA: 2s - loss: 0.1466 - acc: 0.956 - ETA: 2s - loss: 0.1375 - acc: 0.959 - ETA: 2s - loss: 0.1310 - acc: 0.962 - ETA: 2s - loss: 0.1244 - acc: 0.964 - ETA: 1s - loss: 0.1177 - acc: 0.966 - ETA: 1s - loss: 0.1120 - acc: 0.968 - ETA: 1s - loss: 0.1079 - acc: 0.970 - ETA: 1s - loss: 0.1038 - acc: 0.971 - ETA: 1s - loss: 0.1007 - acc: 0.972 - ETA: 1s - loss: 0.0966 - acc: 0.973 - ETA: 1s - loss: 0.0926 - acc: 0.974 - ETA: 1s - loss: 0.0894 - acc: 0.975 - ETA: 1s - loss: 0.0860 - acc: 0.977 - ETA: 1s - loss: 0.0831 - acc: 0.978 - ETA: 0s - loss: 0.0799 - acc: 0.978 - ETA: 0s - loss: 0.0776 - acc: 0.979 - ETA: 0s - loss: 0.0756 - acc: 0.979 - ETA: 0s - loss: 0.0736 - acc: 0.980 - ETA: 0s - loss: 0.0715 - acc: 0.981 - ETA: 0s - loss: 0.0701 - acc: 0.981 - ETA: 0s - loss: 0.0688 - acc: 0.981 - ETA: 0s - loss: 0.0674 - acc: 0.982 - ETA: 0s - loss: 0.0663 - acc: 0.982 - ETA: 0s - loss: 0.0651 - acc: 0.982 - ETA: 0s - loss: 0.0638 - acc: 0.983 - ETA: 0s - loss: 0.0626 - acc: 0.983 - ETA: 0s - loss: 0.0615 - acc: 0.983 - ETA: 0s - loss: 0.0606 - acc: 0.983 - ETA: 0s - loss: 0.0596 - acc: 0.984 - 3s 129us/step - loss: 0.0594 - acc: 0.9841 - val_loss: 0.0167 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01668, saving model to model_weights/adadelta_0.3.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0078 - acc: 1.000 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0150 - acc: 0.995 - ETA: 2s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0225 - acc: 0.993 - ETA: 1s - loss: 0.0230 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0205 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.996 - ETA: 1s - loss: 0.0181 - acc: 0.996 - ETA: 1s - loss: 0.0171 - acc: 0.996 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0175 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.996 - ETA: 1s - loss: 0.0171 - acc: 0.996 - ETA: 1s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.996 - ETA: 0s - loss: 0.0168 - acc: 0.996 - ETA: 0s - loss: 0.0167 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - 2s 115us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0171 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0243 - acc: 0.992 - ETA: 2s - loss: 0.0276 - acc: 0.990 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0140 - acc: 0.996 - ETA: 2s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.995 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.995 - ETA: 1s - loss: 0.0124 - acc: 0.995 - ETA: 1s - loss: 0.0123 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 0s - loss: 0.0132 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0132 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0132 - acc: 0.995 - ETA: 0s - loss: 0.0133 - acc: 0.995 - ETA: 0s - loss: 0.0130 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - 2s 111us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0115 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01668 to 0.01151, saving model to model_weights/adadelta_0.3.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.997 - ETA: 1s - loss: 0.0122 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.997 - ETA: 0s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0110 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.996 - 2s 111us/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0114 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01151 to 0.01142, saving model to model_weights/adadelta_0.3.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0063 - acc: 1.000 - ETA: 2s - loss: 0.0216 - acc: 0.996 - ETA: 2s - loss: 0.0149 - acc: 0.997 - ETA: 2s - loss: 0.0146 - acc: 0.996 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.997 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - 2s 114us/step - loss: 0.0097 - acc: 0.9973 - val_loss: 0.0117 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0037 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0090 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0120 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0092 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0096 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - 2s 109us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0120 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0093 - acc: 0.997 - ETA: 2s - loss: 0.0087 - acc: 0.996 - ETA: 2s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - 2s 108us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0120 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0207 - acc: 0.992 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0097 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - 2s 106us/step - loss: 0.0073 - acc: 0.9981 - val_loss: 0.0132 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0071 - acc: 1.000 - ETA: 2s - loss: 0.0107 - acc: 0.995 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0069 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.997 - 2s 106us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0142 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0364 - acc: 0.984 - ETA: 2s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0093 - acc: 0.996 - ETA: 1s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.996 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - 2s 104us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0121 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0039 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0067 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - 2s 107us/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0131 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 2.4843e-04 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.0000    - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.996 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - 2s 105us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0127 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0022 - acc: 1.000 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - 2s 105us/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0131 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0054 - acc: 0.997 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - 2s 107us/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0129 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0052 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0038 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - 2s 104us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0127 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 0.0088 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0038 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - 2s 104us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0128 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0044 - acc: 1.000 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - 2s 105us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0136 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 2.1108e-04 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.0000    - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0040 - acc: 0.998 - 2s 108us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0141 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 4.8463e-04 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.0000    - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.997 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.999 - ETA: 0s - loss: 0.0036 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.998 - 2s 106us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0139 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0073 - acc: 0.992 - ETA: 2s - loss: 0.0039 - acc: 0.996 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0051 - acc: 0.997 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0033 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.999 - 2s 104us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0133 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 30 - ETA: 0 - ETA:  - ETA:  - 0s 21us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 54s - loss: 0.8401 - acc: 0.36 - ETA: 12s - loss: 0.5594 - acc: 0.67 - ETA: 7s - loss: 0.4071 - acc: 0.7986 - ETA: 5s - loss: 0.3143 - acc: 0.856 - ETA: 4s - loss: 0.2431 - acc: 0.894 - ETA: 3s - loss: 0.1991 - acc: 0.915 - ETA: 3s - loss: 0.1753 - acc: 0.926 - ETA: 2s - loss: 0.1578 - acc: 0.934 - ETA: 2s - loss: 0.1436 - acc: 0.941 - ETA: 2s - loss: 0.1323 - acc: 0.946 - ETA: 2s - loss: 0.1225 - acc: 0.951 - ETA: 2s - loss: 0.1150 - acc: 0.954 - ETA: 2s - loss: 0.1082 - acc: 0.957 - ETA: 1s - loss: 0.1019 - acc: 0.960 - ETA: 1s - loss: 0.0966 - acc: 0.962 - ETA: 1s - loss: 0.0927 - acc: 0.964 - ETA: 1s - loss: 0.0884 - acc: 0.966 - ETA: 1s - loss: 0.0840 - acc: 0.968 - ETA: 1s - loss: 0.0799 - acc: 0.969 - ETA: 1s - loss: 0.0768 - acc: 0.971 - ETA: 1s - loss: 0.0745 - acc: 0.972 - ETA: 1s - loss: 0.0713 - acc: 0.973 - ETA: 0s - loss: 0.0697 - acc: 0.974 - ETA: 0s - loss: 0.0669 - acc: 0.975 - ETA: 0s - loss: 0.0648 - acc: 0.976 - ETA: 0s - loss: 0.0632 - acc: 0.976 - ETA: 0s - loss: 0.0615 - acc: 0.977 - ETA: 0s - loss: 0.0600 - acc: 0.978 - ETA: 0s - loss: 0.0582 - acc: 0.978 - ETA: 0s - loss: 0.0565 - acc: 0.979 - ETA: 0s - loss: 0.0553 - acc: 0.979 - ETA: 0s - loss: 0.0544 - acc: 0.980 - ETA: 0s - loss: 0.0533 - acc: 0.980 - ETA: 0s - loss: 0.0522 - acc: 0.981 - ETA: 0s - loss: 0.0516 - acc: 0.981 - ETA: 0s - loss: 0.0507 - acc: 0.981 - 2s 122us/step - loss: 0.0498 - acc: 0.9821 - val_loss: 0.0156 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01557, saving model to model_weights/adam_0.3.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0212 - acc: 0.992 - ETA: 1s - loss: 0.0107 - acc: 0.998 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - 2s 111us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0127 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01557 to 0.01267, saving model to model_weights/adam_0.3.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0091 - acc: 0.992 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 0s - loss: 0.0127 - acc: 0.995 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.995 - ETA: 0s - loss: 0.0120 - acc: 0.995 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - 2s 102us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0117 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01267 to 0.01168, saving model to model_weights/adam_0.3.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.995 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0111 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0120 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - ETA: 0s - loss: 0.0110 - acc: 0.996 - ETA: 0s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.996 - ETA: 0s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0108 - acc: 0.996 - ETA: 0s - loss: 0.0109 - acc: 0.996 - ETA: 0s - loss: 0.0111 - acc: 0.996 - 2s 101us/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0136 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0100 - acc: 0.996 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0102 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.997 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 1s - loss: 0.0096 - acc: 0.996 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - 2s 101us/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0199 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0124 - acc: 0.992 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0099 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0096 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.996 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - 2s 106us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0136 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0170 - acc: 0.992 - ETA: 1s - loss: 0.0102 - acc: 0.994 - ETA: 1s - loss: 0.0100 - acc: 0.995 - ETA: 1s - loss: 0.0096 - acc: 0.996 - ETA: 1s - loss: 0.0082 - acc: 0.996 - ETA: 1s - loss: 0.0085 - acc: 0.996 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0068 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0069 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0074 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - 2s 98us/step - loss: 0.0075 - acc: 0.9973 - val_loss: 0.0122 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0094 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0072 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - ETA: 0s - loss: 0.0071 - acc: 0.997 - ETA: 0s - loss: 0.0070 - acc: 0.997 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - 2s 108us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0130 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - 2s 97us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0121 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.997 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - 2s 97us/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - 2s 95us/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0129 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 5.1169e-04 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 0.9984    - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - 2s 97us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0126 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0033 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - 2s 97us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0128 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0016 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - 2s 102us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0136 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 5.5675e-04 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.0000    - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - 2s 106us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0137 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 3.3749e-04 - acc: 1.000 - ETA: 1s - loss: 5.1291e-04 - acc: 1.000 - ETA: 1s - loss: 0.0026 - acc: 0.9986    - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.998 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0039 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0038 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.998 - ETA: 0s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0036 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.999 - 2s 103us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0134 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0214 - acc: 0.992 - ETA: 2s - loss: 0.0067 - acc: 0.996 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0036 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0036 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.998 - ETA: 0s - loss: 0.0035 - acc: 0.999 - 2s 103us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0136 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 7.7339e-04 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 1.0000    - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0018 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 1s - loss: 0.0020 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - 2s 103us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0142 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 1.7918e-04 - acc: 1.000 - ETA: 2s - loss: 0.0064 - acc: 0.9984    - ETA: 2s - loss: 0.0048 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - 2s 110us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0140 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0051 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0025 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0025 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0025 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0028 - acc: 0.999 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - 2s 105us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0159 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 45 - ETA: 0 - ETA:  - ETA:  - 0s 25us/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 56s - loss: 0.6902 - acc: 0.60 - ETA: 12s - loss: 0.4663 - acc: 0.81 - ETA: 7s - loss: 0.3679 - acc: 0.8819 - ETA: 5s - loss: 0.3138 - acc: 0.909 - ETA: 4s - loss: 0.2740 - acc: 0.926 - ETA: 3s - loss: 0.2396 - acc: 0.939 - ETA: 3s - loss: 0.2171 - acc: 0.947 - ETA: 3s - loss: 0.2024 - acc: 0.951 - ETA: 2s - loss: 0.1888 - acc: 0.955 - ETA: 2s - loss: 0.1761 - acc: 0.960 - ETA: 2s - loss: 0.1652 - acc: 0.962 - ETA: 2s - loss: 0.1576 - acc: 0.964 - ETA: 2s - loss: 0.1505 - acc: 0.966 - ETA: 1s - loss: 0.1444 - acc: 0.967 - ETA: 1s - loss: 0.1386 - acc: 0.968 - ETA: 1s - loss: 0.1333 - acc: 0.970 - ETA: 1s - loss: 0.1274 - acc: 0.971 - ETA: 1s - loss: 0.1238 - acc: 0.972 - ETA: 1s - loss: 0.1205 - acc: 0.973 - ETA: 1s - loss: 0.1162 - acc: 0.974 - ETA: 1s - loss: 0.1124 - acc: 0.975 - ETA: 1s - loss: 0.1099 - acc: 0.975 - ETA: 1s - loss: 0.1068 - acc: 0.976 - ETA: 0s - loss: 0.1040 - acc: 0.977 - ETA: 0s - loss: 0.1020 - acc: 0.977 - ETA: 0s - loss: 0.0998 - acc: 0.978 - ETA: 0s - loss: 0.0977 - acc: 0.978 - ETA: 0s - loss: 0.0958 - acc: 0.979 - ETA: 0s - loss: 0.0939 - acc: 0.979 - ETA: 0s - loss: 0.0929 - acc: 0.979 - ETA: 0s - loss: 0.0913 - acc: 0.979 - ETA: 0s - loss: 0.0902 - acc: 0.980 - ETA: 0s - loss: 0.0883 - acc: 0.980 - ETA: 0s - loss: 0.0872 - acc: 0.980 - ETA: 0s - loss: 0.0854 - acc: 0.981 - ETA: 0s - loss: 0.0840 - acc: 0.981 - ETA: 0s - loss: 0.0825 - acc: 0.981 - 3s 127us/step - loss: 0.0815 - acc: 0.9821 - val_loss: 0.0337 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03368, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0566 - acc: 0.992 - ETA: 2s - loss: 0.0460 - acc: 0.992 - ETA: 2s - loss: 0.0424 - acc: 0.991 - ETA: 2s - loss: 0.0408 - acc: 0.991 - ETA: 1s - loss: 0.0389 - acc: 0.990 - ETA: 1s - loss: 0.0380 - acc: 0.990 - ETA: 1s - loss: 0.0390 - acc: 0.990 - ETA: 1s - loss: 0.0406 - acc: 0.989 - ETA: 1s - loss: 0.0382 - acc: 0.990 - ETA: 1s - loss: 0.0372 - acc: 0.990 - ETA: 1s - loss: 0.0363 - acc: 0.990 - ETA: 1s - loss: 0.0362 - acc: 0.990 - ETA: 1s - loss: 0.0357 - acc: 0.991 - ETA: 1s - loss: 0.0366 - acc: 0.991 - ETA: 1s - loss: 0.0360 - acc: 0.991 - ETA: 1s - loss: 0.0366 - acc: 0.991 - ETA: 1s - loss: 0.0365 - acc: 0.991 - ETA: 1s - loss: 0.0364 - acc: 0.991 - ETA: 1s - loss: 0.0356 - acc: 0.991 - ETA: 1s - loss: 0.0354 - acc: 0.991 - ETA: 1s - loss: 0.0358 - acc: 0.991 - ETA: 1s - loss: 0.0351 - acc: 0.991 - ETA: 1s - loss: 0.0353 - acc: 0.991 - ETA: 1s - loss: 0.0344 - acc: 0.992 - ETA: 1s - loss: 0.0355 - acc: 0.991 - ETA: 1s - loss: 0.0357 - acc: 0.991 - ETA: 1s - loss: 0.0352 - acc: 0.991 - ETA: 0s - loss: 0.0348 - acc: 0.991 - ETA: 0s - loss: 0.0346 - acc: 0.991 - ETA: 0s - loss: 0.0343 - acc: 0.991 - ETA: 0s - loss: 0.0345 - acc: 0.991 - ETA: 0s - loss: 0.0342 - acc: 0.991 - ETA: 0s - loss: 0.0338 - acc: 0.991 - ETA: 0s - loss: 0.0337 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0336 - acc: 0.991 - ETA: 0s - loss: 0.0336 - acc: 0.991 - ETA: 0s - loss: 0.0333 - acc: 0.991 - ETA: 0s - loss: 0.0334 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0337 - acc: 0.991 - ETA: 0s - loss: 0.0333 - acc: 0.991 - ETA: 0s - loss: 0.0333 - acc: 0.991 - 3s 130us/step - loss: 0.0332 - acc: 0.9916 - val_loss: 0.0247 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03368 to 0.02472, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0467 - acc: 0.984 - ETA: 2s - loss: 0.0330 - acc: 0.992 - ETA: 2s - loss: 0.0290 - acc: 0.994 - ETA: 2s - loss: 0.0291 - acc: 0.992 - ETA: 2s - loss: 0.0282 - acc: 0.992 - ETA: 2s - loss: 0.0266 - acc: 0.993 - ETA: 1s - loss: 0.0260 - acc: 0.993 - ETA: 1s - loss: 0.0255 - acc: 0.994 - ETA: 1s - loss: 0.0265 - acc: 0.993 - ETA: 1s - loss: 0.0259 - acc: 0.994 - ETA: 1s - loss: 0.0257 - acc: 0.994 - ETA: 1s - loss: 0.0253 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0265 - acc: 0.993 - ETA: 1s - loss: 0.0262 - acc: 0.993 - ETA: 1s - loss: 0.0270 - acc: 0.993 - ETA: 1s - loss: 0.0262 - acc: 0.993 - ETA: 1s - loss: 0.0257 - acc: 0.993 - ETA: 1s - loss: 0.0257 - acc: 0.993 - ETA: 1s - loss: 0.0272 - acc: 0.993 - ETA: 1s - loss: 0.0266 - acc: 0.993 - ETA: 1s - loss: 0.0276 - acc: 0.993 - ETA: 0s - loss: 0.0273 - acc: 0.993 - ETA: 0s - loss: 0.0271 - acc: 0.993 - ETA: 0s - loss: 0.0272 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0272 - acc: 0.993 - ETA: 0s - loss: 0.0275 - acc: 0.993 - ETA: 0s - loss: 0.0275 - acc: 0.993 - ETA: 0s - loss: 0.0274 - acc: 0.993 - ETA: 0s - loss: 0.0275 - acc: 0.993 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.993 - ETA: 0s - loss: 0.0276 - acc: 0.993 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0283 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.993 - 2s 120us/step - loss: 0.0276 - acc: 0.9931 - val_loss: 0.0213 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02472 to 0.02127, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0315 - acc: 0.992 - ETA: 2s - loss: 0.0224 - acc: 0.995 - ETA: 2s - loss: 0.0228 - acc: 0.993 - ETA: 2s - loss: 0.0243 - acc: 0.994 - ETA: 2s - loss: 0.0235 - acc: 0.994 - ETA: 2s - loss: 0.0233 - acc: 0.994 - ETA: 2s - loss: 0.0232 - acc: 0.994 - ETA: 1s - loss: 0.0217 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.995 - ETA: 1s - loss: 0.0200 - acc: 0.995 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0227 - acc: 0.993 - ETA: 1s - loss: 0.0223 - acc: 0.993 - ETA: 1s - loss: 0.0225 - acc: 0.993 - ETA: 1s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0232 - acc: 0.993 - ETA: 1s - loss: 0.0232 - acc: 0.993 - ETA: 1s - loss: 0.0232 - acc: 0.993 - ETA: 1s - loss: 0.0235 - acc: 0.993 - ETA: 1s - loss: 0.0232 - acc: 0.993 - ETA: 1s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0238 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0245 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - 2s 120us/step - loss: 0.0239 - acc: 0.9935 - val_loss: 0.0190 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02127 to 0.01899, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0083 - acc: 1.000 - ETA: 2s - loss: 0.0136 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 1s - loss: 0.0223 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0222 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.994 - 2s 115us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0178 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01899 to 0.01780, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0166 - acc: 0.992 - ETA: 1s - loss: 0.0235 - acc: 0.993 - ETA: 2s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0214 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0201 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0200 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0189 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.995 - ETA: 0s - loss: 0.0190 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.995 - ETA: 0s - loss: 0.0186 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.995 - ETA: 0s - loss: 0.0190 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0191 - acc: 0.995 - ETA: 0s - loss: 0.0190 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.995 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0206 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - 2s 105us/step - loss: 0.0204 - acc: 0.9948 - val_loss: 0.0169 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01780 to 0.01690, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0067 - acc: 1.000 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0201 - acc: 0.992 - ETA: 1s - loss: 0.0185 - acc: 0.993 - ETA: 1s - loss: 0.0191 - acc: 0.993 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - 2s 105us/step - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01690 to 0.01616, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0189 - acc: 0.992 - ETA: 2s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - 2s 106us/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0156 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01616 to 0.01564, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0073 - acc: 1.000 - ETA: 2s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0190 - acc: 0.995 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - 2s 106us/step - loss: 0.0175 - acc: 0.9950 - val_loss: 0.0154 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01564 to 0.01539, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0222 - acc: 0.984 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 1s - loss: 0.0164 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0194 - acc: 0.993 - ETA: 1s - loss: 0.0202 - acc: 0.993 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0190 - acc: 0.993 - ETA: 1s - loss: 0.0189 - acc: 0.993 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - 2s 107us/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0152 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01539 to 0.01518, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0090 - acc: 1.000 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0145 - acc: 0.997 - ETA: 2s - loss: 0.0154 - acc: 0.997 - ETA: 2s - loss: 0.0187 - acc: 0.996 - ETA: 2s - loss: 0.0167 - acc: 0.997 - ETA: 2s - loss: 0.0161 - acc: 0.996 - ETA: 2s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0155 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.996 - ETA: 1s - loss: 0.0167 - acc: 0.996 - ETA: 1s - loss: 0.0169 - acc: 0.996 - ETA: 0s - loss: 0.0171 - acc: 0.996 - ETA: 0s - loss: 0.0170 - acc: 0.996 - ETA: 0s - loss: 0.0167 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - 2s 124us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0146 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01518 to 0.01462, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0866 - acc: 0.976 - ETA: 2s - loss: 0.0321 - acc: 0.987 - ETA: 2s - loss: 0.0265 - acc: 0.990 - ETA: 2s - loss: 0.0203 - acc: 0.992 - ETA: 2s - loss: 0.0194 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0193 - acc: 0.993 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.993 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.993 - ETA: 1s - loss: 0.0187 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - 2s 122us/step - loss: 0.0162 - acc: 0.9950 - val_loss: 0.0146 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01462 to 0.01456, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 3s - loss: 0.0195 - acc: 0.992 - ETA: 2s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0158 - acc: 0.994 - ETA: 2s - loss: 0.0139 - acc: 0.995 - ETA: 2s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0180 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0158 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - 2s 119us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0142 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01456 to 0.01423, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0105 - acc: 0.998 - ETA: 2s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0157 - acc: 0.997 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.996 - 2s 110us/step - loss: 0.0153 - acc: 0.9960 - val_loss: 0.0143 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 2s - loss: 0.0157 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.996 - ETA: 2s - loss: 0.0150 - acc: 0.997 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0128 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - 2s 113us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.0139 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01423 to 0.01392, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0050 - acc: 1.000 - ETA: 2s - loss: 0.0084 - acc: 0.998 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 2s - loss: 0.0184 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0128 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0131 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - 2s 117us/step - loss: 0.0148 - acc: 0.9960 - val_loss: 0.0141 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0533 - acc: 0.984 - ETA: 2s - loss: 0.0186 - acc: 0.996 - ETA: 2s - loss: 0.0145 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0132 - acc: 0.996 - ETA: 2s - loss: 0.0127 - acc: 0.996 - ETA: 2s - loss: 0.0130 - acc: 0.996 - ETA: 2s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - 2s 124us/step - loss: 0.0149 - acc: 0.9960 - val_loss: 0.0138 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01392 to 0.01379, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0069 - acc: 1.000 - ETA: 2s - loss: 0.0155 - acc: 0.996 - ETA: 2s - loss: 0.0120 - acc: 0.998 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0105 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0105 - acc: 0.998 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - 2s 114us/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0137 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01379 to 0.01373, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0082 - acc: 1.000 - ETA: 2s - loss: 0.0092 - acc: 1.000 - ETA: 2s - loss: 0.0082 - acc: 1.000 - ETA: 2s - loss: 0.0141 - acc: 0.998 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0122 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.996 - 2s 110us/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.0135 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01373 to 0.01345, saving model to model_weights/sgd_0.3.h5\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 4s - loss: 0.0072 - acc: 1.000 - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - 2s 112us/step - loss: 0.0133 - acc: 0.9964 - val_loss: 0.0138 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 36 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 26us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_predict_model(0.3, 'adadelta')\n",
    "train_predict_model(0.3, 'adam')\n",
    "train_predict_model(0.3, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 58s - loss: 0.9905 - acc: 0.49 - ETA: 13s - loss: 0.6492 - acc: 0.68 - ETA: 8s - loss: 0.4996 - acc: 0.7674 - ETA: 5s - loss: 0.4049 - acc: 0.822 - ETA: 4s - loss: 0.3383 - acc: 0.858 - ETA: 4s - loss: 0.2959 - acc: 0.879 - ETA: 3s - loss: 0.2692 - acc: 0.892 - ETA: 3s - loss: 0.2461 - acc: 0.904 - ETA: 3s - loss: 0.2260 - acc: 0.914 - ETA: 2s - loss: 0.2106 - acc: 0.921 - ETA: 2s - loss: 0.1967 - acc: 0.928 - ETA: 2s - loss: 0.1850 - acc: 0.932 - ETA: 2s - loss: 0.1749 - acc: 0.936 - ETA: 2s - loss: 0.1666 - acc: 0.940 - ETA: 1s - loss: 0.1583 - acc: 0.943 - ETA: 1s - loss: 0.1517 - acc: 0.946 - ETA: 1s - loss: 0.1458 - acc: 0.949 - ETA: 1s - loss: 0.1402 - acc: 0.951 - ETA: 1s - loss: 0.1345 - acc: 0.954 - ETA: 1s - loss: 0.1296 - acc: 0.956 - ETA: 1s - loss: 0.1270 - acc: 0.957 - ETA: 1s - loss: 0.1237 - acc: 0.958 - ETA: 1s - loss: 0.1196 - acc: 0.960 - ETA: 1s - loss: 0.1161 - acc: 0.961 - ETA: 1s - loss: 0.1134 - acc: 0.962 - ETA: 0s - loss: 0.1104 - acc: 0.963 - ETA: 0s - loss: 0.1086 - acc: 0.964 - ETA: 0s - loss: 0.1064 - acc: 0.965 - ETA: 0s - loss: 0.1037 - acc: 0.966 - ETA: 0s - loss: 0.1012 - acc: 0.967 - ETA: 0s - loss: 0.0983 - acc: 0.968 - ETA: 0s - loss: 0.0963 - acc: 0.968 - ETA: 0s - loss: 0.0947 - acc: 0.969 - ETA: 0s - loss: 0.0934 - acc: 0.969 - ETA: 0s - loss: 0.0918 - acc: 0.970 - ETA: 0s - loss: 0.0900 - acc: 0.971 - ETA: 0s - loss: 0.0885 - acc: 0.971 - ETA: 0s - loss: 0.0871 - acc: 0.972 - ETA: 0s - loss: 0.0859 - acc: 0.972 - ETA: 0s - loss: 0.0849 - acc: 0.973 - 3s 134us/step - loss: 0.0844 - acc: 0.9732 - val_loss: 0.0178 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01783, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 3s - loss: 0.0332 - acc: 0.984 - ETA: 2s - loss: 0.0202 - acc: 0.995 - ETA: 2s - loss: 0.0273 - acc: 0.992 - ETA: 2s - loss: 0.0285 - acc: 0.992 - ETA: 2s - loss: 0.0286 - acc: 0.992 - ETA: 2s - loss: 0.0269 - acc: 0.992 - ETA: 1s - loss: 0.0304 - acc: 0.991 - ETA: 1s - loss: 0.0303 - acc: 0.991 - ETA: 1s - loss: 0.0291 - acc: 0.991 - ETA: 1s - loss: 0.0288 - acc: 0.992 - ETA: 1s - loss: 0.0300 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0289 - acc: 0.991 - ETA: 1s - loss: 0.0285 - acc: 0.992 - ETA: 1s - loss: 0.0283 - acc: 0.992 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.992 - ETA: 1s - loss: 0.0287 - acc: 0.991 - ETA: 1s - loss: 0.0287 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0271 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.991 - ETA: 0s - loss: 0.0274 - acc: 0.991 - ETA: 0s - loss: 0.0270 - acc: 0.992 - ETA: 0s - loss: 0.0271 - acc: 0.992 - ETA: 0s - loss: 0.0274 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0269 - acc: 0.992 - ETA: 0s - loss: 0.0269 - acc: 0.992 - 2s 112us/step - loss: 0.0269 - acc: 0.9923 - val_loss: 0.0143 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01783 to 0.01426, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0089 - acc: 1.000 - ETA: 2s - loss: 0.0122 - acc: 0.996 - ETA: 2s - loss: 0.0112 - acc: 0.998 - ETA: 2s - loss: 0.0170 - acc: 0.997 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0193 - acc: 0.995 - ETA: 1s - loss: 0.0191 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.995 - ETA: 1s - loss: 0.0197 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0223 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - 2s 112us/step - loss: 0.0216 - acc: 0.9937 - val_loss: 0.0125 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01426 to 0.01247, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0701 - acc: 0.992 - ETA: 2s - loss: 0.0247 - acc: 0.996 - ETA: 2s - loss: 0.0145 - acc: 0.998 - ETA: 2s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0189 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.994 - ETA: 1s - loss: 0.0200 - acc: 0.994 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - 2s 111us/step - loss: 0.0198 - acc: 0.9941 - val_loss: 0.0115 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01247 to 0.01153, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0625 - acc: 0.984 - ETA: 2s - loss: 0.0156 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.994 - ETA: 2s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.992 - ETA: 1s - loss: 0.0197 - acc: 0.992 - ETA: 1s - loss: 0.0191 - acc: 0.992 - ETA: 1s - loss: 0.0184 - acc: 0.992 - ETA: 1s - loss: 0.0180 - acc: 0.992 - ETA: 1s - loss: 0.0180 - acc: 0.993 - ETA: 1s - loss: 0.0174 - acc: 0.993 - ETA: 1s - loss: 0.0186 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.992 - ETA: 1s - loss: 0.0186 - acc: 0.992 - ETA: 1s - loss: 0.0181 - acc: 0.993 - ETA: 1s - loss: 0.0189 - acc: 0.993 - ETA: 1s - loss: 0.0180 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - 2s 113us/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0124 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0358 - acc: 0.984 - ETA: 2s - loss: 0.0394 - acc: 0.990 - ETA: 2s - loss: 0.0275 - acc: 0.993 - ETA: 2s - loss: 0.0214 - acc: 0.995 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.993 - ETA: 1s - loss: 0.0210 - acc: 0.993 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0255 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.992 - ETA: 1s - loss: 0.0246 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.992 - ETA: 1s - loss: 0.0230 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0226 - acc: 0.992 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.992 - ETA: 1s - loss: 0.0213 - acc: 0.993 - ETA: 1s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.993 - ETA: 0s - loss: 0.0201 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - 2s 108us/step - loss: 0.0198 - acc: 0.9942 - val_loss: 0.0127 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0415 - acc: 0.992 - ETA: 2s - loss: 0.0387 - acc: 0.989 - ETA: 2s - loss: 0.0268 - acc: 0.992 - ETA: 2s - loss: 0.0198 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - 2s 108us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0116 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0073 - acc: 1.000 - ETA: 2s - loss: 0.0263 - acc: 0.995 - ETA: 2s - loss: 0.0221 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0154 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - 2s 105us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0111 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01153 to 0.01108, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0017 - acc: 1.000 - ETA: 1s - loss: 0.0118 - acc: 0.994 - ETA: 1s - loss: 0.0143 - acc: 0.993 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0130 - acc: 0.995 - ETA: 0s - loss: 0.0131 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - 2s 106us/step - loss: 0.0164 - acc: 0.9950 - val_loss: 0.0112 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0247 - acc: 0.992 - ETA: 2s - loss: 0.0155 - acc: 0.992 - ETA: 1s - loss: 0.0133 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0156 - acc: 0.994 - ETA: 1s - loss: 0.0154 - acc: 0.994 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0153 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - 2s 107us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0114 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0237 - acc: 0.992 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - 2s 102us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0113 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0142 - acc: 0.992 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0095 - acc: 0.995 - ETA: 1s - loss: 0.0093 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - 2s 103us/step - loss: 0.0176 - acc: 0.9948 - val_loss: 0.0111 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0148 - acc: 0.992 - ETA: 2s - loss: 0.0078 - acc: 0.996 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - 2s 104us/step - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0116 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0110 - acc: 0.992 - ETA: 2s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0154 - acc: 0.994 - ETA: 1s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0152 - acc: 0.994 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - 2s 106us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0112 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0105 - acc: 0.992 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - 2s 102us/step - loss: 0.0146 - acc: 0.9960 - val_loss: 0.0115 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0654 - acc: 0.976 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0226 - acc: 0.993 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - 2s 104us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0113 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0125 - acc: 0.992 - ETA: 2s - loss: 0.0219 - acc: 0.995 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.995 - ETA: 1s - loss: 0.0217 - acc: 0.994 - ETA: 1s - loss: 0.0222 - acc: 0.994 - ETA: 1s - loss: 0.0221 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.995 - ETA: 1s - loss: 0.0197 - acc: 0.995 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0190 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - 2s 104us/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0123 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0083 - acc: 0.992 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0119 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - 2s 105us/step - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0114 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 9.9907e-04 - acc: 1.000 - ETA: 1s - loss: 0.0162 - acc: 0.9948    - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0184 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.994 - 2s 105us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0114 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 3.6640e-04 - acc: 1.000 - ETA: 2s - loss: 0.0056 - acc: 0.9984    - ETA: 1s - loss: 0.0053 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0077 - acc: 0.999 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.995 - ETA: 0s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0131 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - 2s 102us/step - loss: 0.0152 - acc: 0.9952 - val_loss: 0.0110 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01108 to 0.01104, saving model to model_weights/adadelta_0.8.h5\n",
      "12500/12500 [==============================] - ETA: 38 - ETA: 0 - ETA:  - ETA:  - 0s 24us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijig\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 1:03 - loss: 1.0611 - acc: 0.390 - ETA: 13s - loss: 0.7796 - acc: 0.567 - ETA: 8s - loss: 0.6087 - acc: 0.6806 - ETA: 6s - loss: 0.4883 - acc: 0.757 - ETA: 5s - loss: 0.4065 - acc: 0.807 - ETA: 4s - loss: 0.3532 - acc: 0.836 - ETA: 3s - loss: 0.3113 - acc: 0.859 - ETA: 3s - loss: 0.2798 - acc: 0.875 - ETA: 3s - loss: 0.2547 - acc: 0.889 - ETA: 2s - loss: 0.2358 - acc: 0.898 - ETA: 2s - loss: 0.2188 - acc: 0.906 - ETA: 2s - loss: 0.2011 - acc: 0.914 - ETA: 2s - loss: 0.1880 - acc: 0.920 - ETA: 2s - loss: 0.1774 - acc: 0.925 - ETA: 1s - loss: 0.1682 - acc: 0.930 - ETA: 1s - loss: 0.1600 - acc: 0.934 - ETA: 1s - loss: 0.1531 - acc: 0.937 - ETA: 1s - loss: 0.1468 - acc: 0.940 - ETA: 1s - loss: 0.1414 - acc: 0.942 - ETA: 1s - loss: 0.1348 - acc: 0.945 - ETA: 1s - loss: 0.1304 - acc: 0.947 - ETA: 1s - loss: 0.1249 - acc: 0.950 - ETA: 1s - loss: 0.1209 - acc: 0.951 - ETA: 0s - loss: 0.1181 - acc: 0.952 - ETA: 0s - loss: 0.1152 - acc: 0.954 - ETA: 0s - loss: 0.1112 - acc: 0.956 - ETA: 0s - loss: 0.1095 - acc: 0.957 - ETA: 0s - loss: 0.1074 - acc: 0.958 - ETA: 0s - loss: 0.1045 - acc: 0.959 - ETA: 0s - loss: 0.1020 - acc: 0.960 - ETA: 0s - loss: 0.0998 - acc: 0.961 - ETA: 0s - loss: 0.0973 - acc: 0.962 - ETA: 0s - loss: 0.0950 - acc: 0.963 - ETA: 0s - loss: 0.0929 - acc: 0.964 - ETA: 0s - loss: 0.0915 - acc: 0.965 - ETA: 0s - loss: 0.0897 - acc: 0.965 - ETA: 0s - loss: 0.0879 - acc: 0.966 - ETA: 0s - loss: 0.0868 - acc: 0.967 - 3s 126us/step - loss: 0.0865 - acc: 0.9672 - val_loss: 0.0176 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01756, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0416 - acc: 0.976 - ETA: 1s - loss: 0.0284 - acc: 0.992 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 1s - loss: 0.0317 - acc: 0.991 - ETA: 1s - loss: 0.0330 - acc: 0.990 - ETA: 1s - loss: 0.0300 - acc: 0.991 - ETA: 1s - loss: 0.0281 - acc: 0.991 - ETA: 1s - loss: 0.0286 - acc: 0.991 - ETA: 1s - loss: 0.0307 - acc: 0.990 - ETA: 1s - loss: 0.0310 - acc: 0.990 - ETA: 1s - loss: 0.0307 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0279 - acc: 0.991 - ETA: 1s - loss: 0.0270 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.993 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.993 - ETA: 0s - loss: 0.0255 - acc: 0.993 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.993 - ETA: 0s - loss: 0.0261 - acc: 0.992 - 2s 104us/step - loss: 0.0260 - acc: 0.9929 - val_loss: 0.0156 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01756 to 0.01565, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0050 - acc: 1.000 - ETA: 1s - loss: 0.0286 - acc: 0.989 - ETA: 1s - loss: 0.0289 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.994 - ETA: 1s - loss: 0.0221 - acc: 0.994 - ETA: 1s - loss: 0.0234 - acc: 0.994 - ETA: 1s - loss: 0.0241 - acc: 0.994 - ETA: 1s - loss: 0.0229 - acc: 0.994 - ETA: 1s - loss: 0.0233 - acc: 0.993 - ETA: 1s - loss: 0.0226 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0200 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.994 - ETA: 0s - loss: 0.0210 - acc: 0.994 - ETA: 0s - loss: 0.0206 - acc: 0.994 - ETA: 0s - loss: 0.0203 - acc: 0.994 - ETA: 0s - loss: 0.0203 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - 2s 105us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0128 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01565 to 0.01279, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0112 - acc: 0.997 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 1s - loss: 0.0214 - acc: 0.991 - ETA: 1s - loss: 0.0243 - acc: 0.991 - ETA: 1s - loss: 0.0219 - acc: 0.992 - ETA: 1s - loss: 0.0216 - acc: 0.992 - ETA: 1s - loss: 0.0204 - acc: 0.992 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 1s - loss: 0.0185 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 0s - loss: 0.0183 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.992 - ETA: 0s - loss: 0.0189 - acc: 0.993 - ETA: 0s - loss: 0.0189 - acc: 0.993 - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0196 - acc: 0.992 - ETA: 0s - loss: 0.0195 - acc: 0.992 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0193 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0187 - acc: 0.993 - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0185 - acc: 0.993 - ETA: 0s - loss: 0.0183 - acc: 0.993 - ETA: 0s - loss: 0.0185 - acc: 0.993 - ETA: 0s - loss: 0.0185 - acc: 0.993 - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0193 - acc: 0.993 - ETA: 0s - loss: 0.0196 - acc: 0.993 - 2s 106us/step - loss: 0.0197 - acc: 0.9931 - val_loss: 0.0121 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01279 to 0.01215, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0190 - acc: 0.984 - ETA: 2s - loss: 0.0106 - acc: 0.995 - ETA: 1s - loss: 0.0123 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0190 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - 2s 106us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0116 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01215 to 0.01161, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - 2s 103us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0124 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0466 - acc: 0.984 - ETA: 2s - loss: 0.0324 - acc: 0.990 - ETA: 2s - loss: 0.0233 - acc: 0.990 - ETA: 1s - loss: 0.0264 - acc: 0.989 - ETA: 1s - loss: 0.0229 - acc: 0.990 - ETA: 1s - loss: 0.0199 - acc: 0.991 - ETA: 1s - loss: 0.0181 - acc: 0.992 - ETA: 1s - loss: 0.0168 - acc: 0.993 - ETA: 1s - loss: 0.0164 - acc: 0.992 - ETA: 1s - loss: 0.0174 - acc: 0.993 - ETA: 1s - loss: 0.0167 - acc: 0.993 - ETA: 1s - loss: 0.0173 - acc: 0.993 - ETA: 1s - loss: 0.0168 - acc: 0.993 - ETA: 1s - loss: 0.0166 - acc: 0.993 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0150 - acc: 0.994 - ETA: 0s - loss: 0.0150 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.993 - ETA: 0s - loss: 0.0187 - acc: 0.993 - ETA: 0s - loss: 0.0186 - acc: 0.993 - ETA: 0s - loss: 0.0186 - acc: 0.993 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - 2s 104us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 0.0123 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0091 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0128 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - 2s 104us/step - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0124 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0293 - acc: 0.984 - ETA: 2s - loss: 0.0137 - acc: 0.992 - ETA: 1s - loss: 0.0104 - acc: 0.995 - ETA: 1s - loss: 0.0086 - acc: 0.996 - ETA: 1s - loss: 0.0092 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.997 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 1s - loss: 0.0091 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0100 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0124 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.995 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 0s - loss: 0.0118 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0117 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.995 - ETA: 0s - loss: 0.0129 - acc: 0.995 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0130 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - 2s 104us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0119 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0041 - acc: 1.000 - ETA: 1s - loss: 0.0303 - acc: 0.992 - ETA: 1s - loss: 0.0230 - acc: 0.993 - ETA: 1s - loss: 0.0258 - acc: 0.991 - ETA: 1s - loss: 0.0235 - acc: 0.992 - ETA: 1s - loss: 0.0211 - acc: 0.992 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0182 - acc: 0.993 - ETA: 1s - loss: 0.0171 - acc: 0.993 - ETA: 1s - loss: 0.0163 - acc: 0.993 - ETA: 1s - loss: 0.0169 - acc: 0.993 - ETA: 1s - loss: 0.0161 - acc: 0.993 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0154 - acc: 0.994 - ETA: 1s - loss: 0.0148 - acc: 0.994 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.994 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - 2s 109us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 0.0132 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0061 - acc: 0.992 - ETA: 1s - loss: 0.0172 - acc: 0.993 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - 2s 103us/step - loss: 0.0161 - acc: 0.9952 - val_loss: 0.0116 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01161 to 0.01157, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0102 - acc: 0.992 - ETA: 2s - loss: 0.0096 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.994 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - 2s 104us/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0122 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0164 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0124 - acc: 0.995 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - 2s 104us/step - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0125 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0152 - acc: 0.992 - ETA: 1s - loss: 0.0177 - acc: 0.992 - ETA: 1s - loss: 0.0178 - acc: 0.992 - ETA: 1s - loss: 0.0182 - acc: 0.992 - ETA: 1s - loss: 0.0179 - acc: 0.993 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.994 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0163 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.994 - 2s 107us/step - loss: 0.0165 - acc: 0.9950 - val_loss: 0.0121 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0194 - acc: 0.992 - ETA: 2s - loss: 0.0126 - acc: 0.993 - ETA: 1s - loss: 0.0102 - acc: 0.995 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0125 - acc: 0.995 - ETA: 1s - loss: 0.0118 - acc: 0.995 - ETA: 1s - loss: 0.0122 - acc: 0.995 - ETA: 1s - loss: 0.0122 - acc: 0.995 - ETA: 1s - loss: 0.0116 - acc: 0.995 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0119 - acc: 0.995 - ETA: 1s - loss: 0.0125 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0130 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0134 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - 2s 104us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0129 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 7.6494e-04 - acc: 1.000 - ETA: 1s - loss: 0.0103 - acc: 0.9961    - ETA: 1s - loss: 0.0145 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.992 - ETA: 1s - loss: 0.0139 - acc: 0.994 - ETA: 1s - loss: 0.0131 - acc: 0.994 - ETA: 1s - loss: 0.0134 - acc: 0.994 - ETA: 1s - loss: 0.0156 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.994 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.994 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.994 - ETA: 0s - loss: 0.0151 - acc: 0.994 - ETA: 0s - loss: 0.0147 - acc: 0.994 - ETA: 0s - loss: 0.0145 - acc: 0.994 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0133 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - 2s 104us/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.0134 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0394 - acc: 0.984 - ETA: 1s - loss: 0.0307 - acc: 0.990 - ETA: 2s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0229 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0208 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - 2s 106us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0118 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0084 - acc: 0.992 - ETA: 2s - loss: 0.0145 - acc: 0.992 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0205 - acc: 0.993 - ETA: 1s - loss: 0.0213 - acc: 0.992 - ETA: 1s - loss: 0.0178 - acc: 0.993 - ETA: 1s - loss: 0.0153 - acc: 0.994 - ETA: 1s - loss: 0.0151 - acc: 0.994 - ETA: 1s - loss: 0.0150 - acc: 0.994 - ETA: 1s - loss: 0.0147 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0164 - acc: 0.994 - ETA: 1s - loss: 0.0153 - acc: 0.994 - ETA: 1s - loss: 0.0150 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - 2s 106us/step - loss: 0.0160 - acc: 0.9946 - val_loss: 0.0111 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01157 to 0.01114, saving model to model_weights/adam_0.8.h5\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0061 - acc: 1.000 - ETA: 2s - loss: 0.0106 - acc: 0.995 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0091 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.994 - ETA: 1s - loss: 0.0126 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0129 - acc: 0.995 - ETA: 1s - loss: 0.0128 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - 2s 106us/step - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0117 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0215 - acc: 0.984 - ETA: 2s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.993 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.994 - ETA: 0s - loss: 0.0153 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.994 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.994 - 2s 104us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0115 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "12500/12500 [==============================] - ETA: 42 - ETA: 0 - ETA:  - ETA:  - 0s 25us/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 1:00 - loss: 0.7680 - acc: 0.570 - ETA: 13s - loss: 0.5782 - acc: 0.695 - ETA: 7s - loss: 0.4425 - acc: 0.7875 - ETA: 5s - loss: 0.3638 - acc: 0.835 - ETA: 4s - loss: 0.3224 - acc: 0.859 - ETA: 3s - loss: 0.2915 - acc: 0.878 - ETA: 3s - loss: 0.2675 - acc: 0.890 - ETA: 3s - loss: 0.2494 - acc: 0.899 - ETA: 2s - loss: 0.2298 - acc: 0.910 - ETA: 2s - loss: 0.2184 - acc: 0.915 - ETA: 2s - loss: 0.2083 - acc: 0.920 - ETA: 2s - loss: 0.1961 - acc: 0.927 - ETA: 1s - loss: 0.1881 - acc: 0.930 - ETA: 1s - loss: 0.1805 - acc: 0.934 - ETA: 1s - loss: 0.1737 - acc: 0.937 - ETA: 1s - loss: 0.1680 - acc: 0.939 - ETA: 1s - loss: 0.1626 - acc: 0.941 - ETA: 1s - loss: 0.1579 - acc: 0.943 - ETA: 1s - loss: 0.1543 - acc: 0.945 - ETA: 1s - loss: 0.1497 - acc: 0.947 - ETA: 1s - loss: 0.1461 - acc: 0.948 - ETA: 1s - loss: 0.1428 - acc: 0.950 - ETA: 0s - loss: 0.1397 - acc: 0.951 - ETA: 0s - loss: 0.1357 - acc: 0.953 - ETA: 0s - loss: 0.1335 - acc: 0.954 - ETA: 0s - loss: 0.1313 - acc: 0.955 - ETA: 0s - loss: 0.1283 - acc: 0.956 - ETA: 0s - loss: 0.1259 - acc: 0.956 - ETA: 0s - loss: 0.1233 - acc: 0.958 - ETA: 0s - loss: 0.1222 - acc: 0.958 - ETA: 0s - loss: 0.1201 - acc: 0.959 - ETA: 0s - loss: 0.1185 - acc: 0.959 - ETA: 0s - loss: 0.1162 - acc: 0.960 - ETA: 0s - loss: 0.1145 - acc: 0.961 - ETA: 0s - loss: 0.1128 - acc: 0.962 - ETA: 0s - loss: 0.1113 - acc: 0.962 - 2s 124us/step - loss: 0.1100 - acc: 0.9632 - val_loss: 0.0304 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03036, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0836 - acc: 0.976 - ETA: 1s - loss: 0.0740 - acc: 0.979 - ETA: 1s - loss: 0.0629 - acc: 0.983 - ETA: 1s - loss: 0.0615 - acc: 0.981 - ETA: 1s - loss: 0.0632 - acc: 0.981 - ETA: 1s - loss: 0.0622 - acc: 0.981 - ETA: 1s - loss: 0.0608 - acc: 0.982 - ETA: 1s - loss: 0.0585 - acc: 0.982 - ETA: 1s - loss: 0.0590 - acc: 0.982 - ETA: 1s - loss: 0.0580 - acc: 0.982 - ETA: 1s - loss: 0.0566 - acc: 0.983 - ETA: 1s - loss: 0.0568 - acc: 0.983 - ETA: 1s - loss: 0.0562 - acc: 0.983 - ETA: 1s - loss: 0.0560 - acc: 0.983 - ETA: 1s - loss: 0.0553 - acc: 0.983 - ETA: 1s - loss: 0.0558 - acc: 0.982 - ETA: 1s - loss: 0.0561 - acc: 0.982 - ETA: 1s - loss: 0.0561 - acc: 0.982 - ETA: 0s - loss: 0.0558 - acc: 0.982 - ETA: 0s - loss: 0.0546 - acc: 0.982 - ETA: 0s - loss: 0.0539 - acc: 0.983 - ETA: 0s - loss: 0.0534 - acc: 0.983 - ETA: 0s - loss: 0.0529 - acc: 0.983 - ETA: 0s - loss: 0.0520 - acc: 0.983 - ETA: 0s - loss: 0.0514 - acc: 0.984 - ETA: 0s - loss: 0.0513 - acc: 0.984 - ETA: 0s - loss: 0.0512 - acc: 0.984 - ETA: 0s - loss: 0.0509 - acc: 0.984 - ETA: 0s - loss: 0.0506 - acc: 0.984 - ETA: 0s - loss: 0.0501 - acc: 0.984 - ETA: 0s - loss: 0.0497 - acc: 0.984 - ETA: 0s - loss: 0.0496 - acc: 0.984 - ETA: 0s - loss: 0.0498 - acc: 0.984 - ETA: 0s - loss: 0.0497 - acc: 0.984 - ETA: 0s - loss: 0.0496 - acc: 0.984 - ETA: 0s - loss: 0.0496 - acc: 0.984 - 2s 102us/step - loss: 0.0492 - acc: 0.9845 - val_loss: 0.0221 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03036 to 0.02206, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0703 - acc: 0.976 - ETA: 1s - loss: 0.0498 - acc: 0.984 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0353 - acc: 0.990 - ETA: 1s - loss: 0.0407 - acc: 0.988 - ETA: 1s - loss: 0.0399 - acc: 0.987 - ETA: 1s - loss: 0.0390 - acc: 0.987 - ETA: 1s - loss: 0.0391 - acc: 0.988 - ETA: 1s - loss: 0.0403 - acc: 0.988 - ETA: 1s - loss: 0.0383 - acc: 0.989 - ETA: 1s - loss: 0.0383 - acc: 0.989 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 1s - loss: 0.0376 - acc: 0.989 - ETA: 1s - loss: 0.0370 - acc: 0.990 - ETA: 1s - loss: 0.0374 - acc: 0.989 - ETA: 1s - loss: 0.0376 - acc: 0.989 - ETA: 1s - loss: 0.0375 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 0s - loss: 0.0379 - acc: 0.989 - ETA: 0s - loss: 0.0388 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0383 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.989 - ETA: 0s - loss: 0.0378 - acc: 0.989 - ETA: 0s - loss: 0.0378 - acc: 0.989 - ETA: 0s - loss: 0.0373 - acc: 0.989 - ETA: 0s - loss: 0.0382 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0390 - acc: 0.988 - ETA: 0s - loss: 0.0387 - acc: 0.988 - ETA: 0s - loss: 0.0391 - acc: 0.988 - ETA: 0s - loss: 0.0388 - acc: 0.988 - ETA: 0s - loss: 0.0388 - acc: 0.988 - ETA: 0s - loss: 0.0390 - acc: 0.988 - ETA: 0s - loss: 0.0386 - acc: 0.988 - 2s 104us/step - loss: 0.0385 - acc: 0.9889 - val_loss: 0.0192 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02206 to 0.01922, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0380 - acc: 0.992 - ETA: 2s - loss: 0.0309 - acc: 0.992 - ETA: 1s - loss: 0.0327 - acc: 0.991 - ETA: 1s - loss: 0.0312 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.990 - ETA: 1s - loss: 0.0329 - acc: 0.989 - ETA: 1s - loss: 0.0322 - acc: 0.990 - ETA: 1s - loss: 0.0330 - acc: 0.990 - ETA: 1s - loss: 0.0331 - acc: 0.990 - ETA: 1s - loss: 0.0356 - acc: 0.989 - ETA: 1s - loss: 0.0364 - acc: 0.989 - ETA: 1s - loss: 0.0359 - acc: 0.989 - ETA: 1s - loss: 0.0364 - acc: 0.988 - ETA: 1s - loss: 0.0365 - acc: 0.988 - ETA: 1s - loss: 0.0354 - acc: 0.989 - ETA: 1s - loss: 0.0348 - acc: 0.989 - ETA: 1s - loss: 0.0352 - acc: 0.989 - ETA: 1s - loss: 0.0357 - acc: 0.989 - ETA: 1s - loss: 0.0364 - acc: 0.989 - ETA: 0s - loss: 0.0352 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.990 - ETA: 0s - loss: 0.0332 - acc: 0.990 - ETA: 0s - loss: 0.0327 - acc: 0.990 - ETA: 0s - loss: 0.0324 - acc: 0.990 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0316 - acc: 0.990 - ETA: 0s - loss: 0.0314 - acc: 0.990 - ETA: 0s - loss: 0.0311 - acc: 0.990 - ETA: 0s - loss: 0.0312 - acc: 0.990 - ETA: 0s - loss: 0.0310 - acc: 0.990 - ETA: 0s - loss: 0.0310 - acc: 0.990 - ETA: 0s - loss: 0.0311 - acc: 0.990 - ETA: 0s - loss: 0.0316 - acc: 0.990 - ETA: 0s - loss: 0.0322 - acc: 0.990 - ETA: 0s - loss: 0.0322 - acc: 0.990 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0326 - acc: 0.990 - 2s 108us/step - loss: 0.0330 - acc: 0.9902 - val_loss: 0.0173 - val_acc: 0.9948\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss improved from 0.01922 to 0.01731, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 5/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0277 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0297 - acc: 0.992 - ETA: 1s - loss: 0.0335 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.990 - ETA: 1s - loss: 0.0311 - acc: 0.989 - ETA: 1s - loss: 0.0302 - acc: 0.989 - ETA: 1s - loss: 0.0293 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.989 - ETA: 1s - loss: 0.0305 - acc: 0.990 - ETA: 1s - loss: 0.0312 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.989 - ETA: 1s - loss: 0.0309 - acc: 0.990 - ETA: 1s - loss: 0.0306 - acc: 0.990 - ETA: 1s - loss: 0.0316 - acc: 0.990 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 1s - loss: 0.0322 - acc: 0.990 - ETA: 0s - loss: 0.0322 - acc: 0.990 - ETA: 0s - loss: 0.0327 - acc: 0.989 - ETA: 0s - loss: 0.0323 - acc: 0.989 - ETA: 0s - loss: 0.0320 - acc: 0.990 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0314 - acc: 0.990 - ETA: 0s - loss: 0.0314 - acc: 0.990 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0316 - acc: 0.990 - ETA: 0s - loss: 0.0314 - acc: 0.990 - ETA: 0s - loss: 0.0312 - acc: 0.990 - ETA: 0s - loss: 0.0315 - acc: 0.990 - ETA: 0s - loss: 0.0311 - acc: 0.990 - ETA: 0s - loss: 0.0313 - acc: 0.990 - ETA: 0s - loss: 0.0308 - acc: 0.990 - ETA: 0s - loss: 0.0307 - acc: 0.990 - ETA: 0s - loss: 0.0306 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - 2s 105us/step - loss: 0.0309 - acc: 0.9905 - val_loss: 0.0166 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01731 to 0.01658, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0134 - acc: 1.000 - ETA: 2s - loss: 0.0264 - acc: 0.990 - ETA: 2s - loss: 0.0297 - acc: 0.988 - ETA: 1s - loss: 0.0304 - acc: 0.988 - ETA: 1s - loss: 0.0336 - acc: 0.988 - ETA: 1s - loss: 0.0301 - acc: 0.990 - ETA: 1s - loss: 0.0296 - acc: 0.990 - ETA: 1s - loss: 0.0317 - acc: 0.990 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.990 - ETA: 1s - loss: 0.0297 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 1s - loss: 0.0327 - acc: 0.990 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 1s - loss: 0.0314 - acc: 0.990 - ETA: 1s - loss: 0.0313 - acc: 0.990 - ETA: 1s - loss: 0.0315 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - ETA: 0s - loss: 0.0310 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - ETA: 0s - loss: 0.0309 - acc: 0.990 - ETA: 0s - loss: 0.0313 - acc: 0.990 - ETA: 0s - loss: 0.0307 - acc: 0.990 - ETA: 0s - loss: 0.0299 - acc: 0.990 - ETA: 0s - loss: 0.0294 - acc: 0.991 - ETA: 0s - loss: 0.0299 - acc: 0.990 - ETA: 0s - loss: 0.0298 - acc: 0.991 - ETA: 0s - loss: 0.0292 - acc: 0.991 - ETA: 0s - loss: 0.0295 - acc: 0.991 - ETA: 0s - loss: 0.0301 - acc: 0.990 - ETA: 0s - loss: 0.0301 - acc: 0.990 - ETA: 0s - loss: 0.0303 - acc: 0.990 - ETA: 0s - loss: 0.0302 - acc: 0.990 - ETA: 0s - loss: 0.0302 - acc: 0.990 - 2s 104us/step - loss: 0.0300 - acc: 0.9908 - val_loss: 0.0162 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01658 to 0.01617, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0287 - acc: 0.984 - ETA: 1s - loss: 0.0366 - acc: 0.987 - ETA: 1s - loss: 0.0261 - acc: 0.993 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0324 - acc: 0.992 - ETA: 1s - loss: 0.0326 - acc: 0.991 - ETA: 1s - loss: 0.0315 - acc: 0.991 - ETA: 1s - loss: 0.0298 - acc: 0.992 - ETA: 1s - loss: 0.0300 - acc: 0.991 - ETA: 1s - loss: 0.0298 - acc: 0.991 - ETA: 1s - loss: 0.0301 - acc: 0.990 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0291 - acc: 0.991 - ETA: 1s - loss: 0.0290 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0295 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0286 - acc: 0.991 - ETA: 0s - loss: 0.0288 - acc: 0.991 - ETA: 0s - loss: 0.0292 - acc: 0.991 - ETA: 0s - loss: 0.0291 - acc: 0.991 - ETA: 0s - loss: 0.0293 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0292 - acc: 0.990 - ETA: 0s - loss: 0.0291 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0300 - acc: 0.990 - ETA: 0s - loss: 0.0300 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0291 - acc: 0.990 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0291 - acc: 0.990 - ETA: 0s - loss: 0.0289 - acc: 0.990 - ETA: 0s - loss: 0.0289 - acc: 0.990 - 2s 102us/step - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0148 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01617 to 0.01479, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0064 - acc: 1.000 - ETA: 2s - loss: 0.0331 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.993 - ETA: 1s - loss: 0.0255 - acc: 0.993 - ETA: 1s - loss: 0.0308 - acc: 0.990 - ETA: 1s - loss: 0.0289 - acc: 0.991 - ETA: 1s - loss: 0.0276 - acc: 0.992 - ETA: 1s - loss: 0.0309 - acc: 0.991 - ETA: 1s - loss: 0.0304 - acc: 0.990 - ETA: 1s - loss: 0.0304 - acc: 0.990 - ETA: 1s - loss: 0.0288 - acc: 0.991 - ETA: 1s - loss: 0.0278 - acc: 0.991 - ETA: 1s - loss: 0.0284 - acc: 0.991 - ETA: 1s - loss: 0.0298 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0286 - acc: 0.991 - ETA: 1s - loss: 0.0287 - acc: 0.991 - ETA: 1s - loss: 0.0286 - acc: 0.991 - ETA: 1s - loss: 0.0281 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0273 - acc: 0.991 - ETA: 0s - loss: 0.0267 - acc: 0.991 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0251 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - 2s 104us/step - loss: 0.0259 - acc: 0.9923 - val_loss: 0.0145 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01479 to 0.01447, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 9/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0167 - acc: 0.992 - ETA: 1s - loss: 0.0221 - acc: 0.989 - ETA: 1s - loss: 0.0229 - acc: 0.989 - ETA: 1s - loss: 0.0284 - acc: 0.989 - ETA: 1s - loss: 0.0313 - acc: 0.989 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0285 - acc: 0.990 - ETA: 1s - loss: 0.0286 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0277 - acc: 0.990 - ETA: 1s - loss: 0.0277 - acc: 0.990 - ETA: 1s - loss: 0.0276 - acc: 0.990 - ETA: 1s - loss: 0.0268 - acc: 0.990 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0263 - acc: 0.991 - ETA: 1s - loss: 0.0268 - acc: 0.991 - ETA: 1s - loss: 0.0268 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0268 - acc: 0.991 - ETA: 0s - loss: 0.0264 - acc: 0.991 - ETA: 0s - loss: 0.0262 - acc: 0.991 - ETA: 0s - loss: 0.0261 - acc: 0.991 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.991 - ETA: 0s - loss: 0.0264 - acc: 0.991 - ETA: 0s - loss: 0.0266 - acc: 0.991 - ETA: 0s - loss: 0.0269 - acc: 0.991 - ETA: 0s - loss: 0.0270 - acc: 0.991 - ETA: 0s - loss: 0.0269 - acc: 0.991 - ETA: 0s - loss: 0.0271 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0272 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.991 - 2s 104us/step - loss: 0.0273 - acc: 0.9916 - val_loss: 0.0145 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0661 - acc: 0.992 - ETA: 2s - loss: 0.0244 - acc: 0.996 - ETA: 1s - loss: 0.0196 - acc: 0.996 - ETA: 1s - loss: 0.0204 - acc: 0.995 - ETA: 1s - loss: 0.0253 - acc: 0.993 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 1s - loss: 0.0239 - acc: 0.993 - ETA: 1s - loss: 0.0249 - acc: 0.993 - ETA: 1s - loss: 0.0249 - acc: 0.993 - ETA: 1s - loss: 0.0260 - acc: 0.993 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0253 - acc: 0.993 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.993 - ETA: 1s - loss: 0.0255 - acc: 0.993 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0264 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0268 - acc: 0.992 - ETA: 0s - loss: 0.0269 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - 2s 102us/step - loss: 0.0259 - acc: 0.9926 - val_loss: 0.0141 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01447 to 0.01407, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0165 - acc: 1.000 - ETA: 2s - loss: 0.0201 - acc: 0.992 - ETA: 1s - loss: 0.0173 - acc: 0.993 - ETA: 1s - loss: 0.0186 - acc: 0.993 - ETA: 1s - loss: 0.0200 - acc: 0.992 - ETA: 1s - loss: 0.0219 - acc: 0.992 - ETA: 1s - loss: 0.0206 - acc: 0.992 - ETA: 1s - loss: 0.0200 - acc: 0.992 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.992 - ETA: 1s - loss: 0.0227 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.993 - ETA: 1s - loss: 0.0236 - acc: 0.993 - ETA: 1s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.992 - 2s 103us/step - loss: 0.0237 - acc: 0.9930 - val_loss: 0.0137 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01407 to 0.01373, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0203 - acc: 0.993 - ETA: 2s - loss: 0.0252 - acc: 0.992 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.991 - ETA: 1s - loss: 0.0280 - acc: 0.990 - ETA: 1s - loss: 0.0276 - acc: 0.990 - ETA: 1s - loss: 0.0264 - acc: 0.991 - ETA: 1s - loss: 0.0274 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0238 - acc: 0.993 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - 2s 108us/step - loss: 0.0251 - acc: 0.9925 - val_loss: 0.0141 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0924 - acc: 0.968 - ETA: 2s - loss: 0.0291 - acc: 0.990 - ETA: 1s - loss: 0.0271 - acc: 0.989 - ETA: 1s - loss: 0.0246 - acc: 0.992 - ETA: 1s - loss: 0.0250 - acc: 0.990 - ETA: 1s - loss: 0.0249 - acc: 0.991 - ETA: 1s - loss: 0.0254 - acc: 0.991 - ETA: 1s - loss: 0.0275 - acc: 0.991 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0277 - acc: 0.991 - ETA: 1s - loss: 0.0273 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0258 - acc: 0.991 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0246 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0230 - acc: 0.992 - ETA: 0s - loss: 0.0229 - acc: 0.992 - ETA: 0s - loss: 0.0231 - acc: 0.992 - ETA: 0s - loss: 0.0230 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - 2s 112us/step - loss: 0.0240 - acc: 0.9927 - val_loss: 0.0135 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01373 to 0.01349, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0190 - acc: 0.992 - ETA: 2s - loss: 0.0355 - acc: 0.990 - ETA: 2s - loss: 0.0272 - acc: 0.992 - ETA: 2s - loss: 0.0265 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.991 - ETA: 1s - loss: 0.0254 - acc: 0.991 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0273 - acc: 0.992 - ETA: 1s - loss: 0.0270 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.993 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0255 - acc: 0.992 - ETA: 1s - loss: 0.0253 - acc: 0.992 - ETA: 1s - loss: 0.0255 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0247 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0242 - acc: 0.993 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0247 - acc: 0.992 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0241 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - 2s 108us/step - loss: 0.0232 - acc: 0.9931 - val_loss: 0.0133 - val_acc: 0.9956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: val_loss improved from 0.01349 to 0.01330, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0140 - acc: 1.000 - ETA: 1s - loss: 0.0090 - acc: 0.998 - ETA: 1s - loss: 0.0128 - acc: 0.997 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.992 - ETA: 1s - loss: 0.0225 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.991 - ETA: 1s - loss: 0.0253 - acc: 0.992 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 1s - loss: 0.0230 - acc: 0.992 - ETA: 1s - loss: 0.0232 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0226 - acc: 0.993 - ETA: 1s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0219 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - 2s 107us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.0131 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01330 to 0.01314, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0316 - acc: 0.984 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0234 - acc: 0.993 - ETA: 1s - loss: 0.0233 - acc: 0.994 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0234 - acc: 0.994 - ETA: 1s - loss: 0.0237 - acc: 0.994 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0236 - acc: 0.993 - ETA: 1s - loss: 0.0227 - acc: 0.994 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0225 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0235 - acc: 0.993 - ETA: 1s - loss: 0.0233 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0243 - acc: 0.992 - ETA: 0s - loss: 0.0241 - acc: 0.992 - ETA: 0s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0229 - acc: 0.992 - ETA: 0s - loss: 0.0226 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.992 - ETA: 0s - loss: 0.0227 - acc: 0.992 - ETA: 0s - loss: 0.0225 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0220 - acc: 0.993 - 2s 108us/step - loss: 0.0220 - acc: 0.9930 - val_loss: 0.0132 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0088 - acc: 1.000 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0227 - acc: 0.993 - ETA: 1s - loss: 0.0259 - acc: 0.993 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0253 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.991 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.991 - ETA: 1s - loss: 0.0252 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0247 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0231 - acc: 0.992 - ETA: 0s - loss: 0.0226 - acc: 0.992 - ETA: 0s - loss: 0.0230 - acc: 0.992 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0231 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.992 - 2s 106us/step - loss: 0.0235 - acc: 0.9928 - val_loss: 0.0130 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01314 to 0.01303, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0135 - acc: 1.000 - ETA: 2s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0174 - acc: 0.996 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.993 - ETA: 1s - loss: 0.0233 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.993 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0210 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0208 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - 2s 105us/step - loss: 0.0219 - acc: 0.9934 - val_loss: 0.0132 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0126 - acc: 0.992 - ETA: 2s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0189 - acc: 0.992 - ETA: 1s - loss: 0.0179 - acc: 0.993 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.993 - ETA: 1s - loss: 0.0177 - acc: 0.993 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0194 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0201 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - 2s 111us/step - loss: 0.0214 - acc: 0.9930 - val_loss: 0.0129 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01303 to 0.01288, saving model to model_weights/sgd_0.8.h5\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0462 - acc: 0.984 - ETA: 2s - loss: 0.0283 - acc: 0.994 - ETA: 2s - loss: 0.0262 - acc: 0.995 - ETA: 2s - loss: 0.0249 - acc: 0.994 - ETA: 1s - loss: 0.0201 - acc: 0.995 - ETA: 2s - loss: 0.0220 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0228 - acc: 0.993 - ETA: 1s - loss: 0.0223 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0197 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0203 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - 2s 108us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.0128 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01288 to 0.01284, saving model to model_weights/sgd_0.8.h5\n",
      "12500/12500 [==============================] - ETA: 45 - ETA: 0 - ETA:  - ETA:  - 0s 26us/step\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_predict_model(0.8, 'adadelta')\n",
    "train_predict_model(0.8, 'adam')\n",
    "train_predict_model(0.8, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dropout, optimizer):\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    x = Dropout(dropout)(input_tensor)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    filepath=f\"model_weights/{optimizer}_{dropout}.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2, shuffle=True,\n",
    "             callbacks=callbacks_list)\n",
    "    model.save_weights(f\"model_weights/{optimizer}_{dropout}.h5\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19968 samples, validate on 4993 samples\n",
      "Epoch 1/20\n",
      "19968/19968 [==============================] - ETA: 1:19 - loss: 0.8216 - acc: 0.531 - ETA: 21s - loss: 0.6298 - acc: 0.664 - ETA: 11s - loss: 0.4721 - acc: 0.77 - ETA: 8s - loss: 0.3895 - acc: 0.8224 - ETA: 6s - loss: 0.3275 - acc: 0.859 - ETA: 5s - loss: 0.2891 - acc: 0.882 - ETA: 4s - loss: 0.2548 - acc: 0.900 - ETA: 4s - loss: 0.2309 - acc: 0.911 - ETA: 3s - loss: 0.2129 - acc: 0.921 - ETA: 3s - loss: 0.1966 - acc: 0.928 - ETA: 3s - loss: 0.1825 - acc: 0.934 - ETA: 3s - loss: 0.1712 - acc: 0.939 - ETA: 2s - loss: 0.1639 - acc: 0.943 - ETA: 2s - loss: 0.1565 - acc: 0.946 - ETA: 2s - loss: 0.1496 - acc: 0.949 - ETA: 2s - loss: 0.1431 - acc: 0.951 - ETA: 2s - loss: 0.1375 - acc: 0.953 - ETA: 1s - loss: 0.1332 - acc: 0.955 - ETA: 1s - loss: 0.1287 - acc: 0.957 - ETA: 1s - loss: 0.1238 - acc: 0.959 - ETA: 1s - loss: 0.1208 - acc: 0.960 - ETA: 1s - loss: 0.1166 - acc: 0.962 - ETA: 1s - loss: 0.1135 - acc: 0.963 - ETA: 1s - loss: 0.1098 - acc: 0.964 - ETA: 1s - loss: 0.1068 - acc: 0.965 - ETA: 1s - loss: 0.1042 - acc: 0.966 - ETA: 1s - loss: 0.1018 - acc: 0.967 - ETA: 0s - loss: 0.0992 - acc: 0.968 - ETA: 0s - loss: 0.0974 - acc: 0.969 - ETA: 0s - loss: 0.0959 - acc: 0.969 - ETA: 0s - loss: 0.0939 - acc: 0.970 - ETA: 0s - loss: 0.0923 - acc: 0.970 - ETA: 0s - loss: 0.0906 - acc: 0.971 - ETA: 0s - loss: 0.0890 - acc: 0.971 - ETA: 0s - loss: 0.0872 - acc: 0.972 - ETA: 0s - loss: 0.0854 - acc: 0.973 - ETA: 0s - loss: 0.0838 - acc: 0.973 - ETA: 0s - loss: 0.0821 - acc: 0.974 - ETA: 0s - loss: 0.0814 - acc: 0.974 - ETA: 0s - loss: 0.0799 - acc: 0.974 - 3s 146us/step - loss: 0.0788 - acc: 0.9752 - val_loss: 0.0176 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01763, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 2/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0151 - acc: 1.000 - ETA: 2s - loss: 0.0268 - acc: 0.992 - ETA: 2s - loss: 0.0254 - acc: 0.992 - ETA: 2s - loss: 0.0253 - acc: 0.992 - ETA: 2s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0303 - acc: 0.992 - ETA: 1s - loss: 0.0286 - acc: 0.992 - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0283 - acc: 0.992 - ETA: 1s - loss: 0.0300 - acc: 0.991 - ETA: 1s - loss: 0.0291 - acc: 0.991 - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0276 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0263 - acc: 0.992 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0263 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0264 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0265 - acc: 0.991 - ETA: 1s - loss: 0.0260 - acc: 0.991 - ETA: 1s - loss: 0.0261 - acc: 0.991 - ETA: 0s - loss: 0.0258 - acc: 0.991 - ETA: 0s - loss: 0.0251 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0247 - acc: 0.992 - ETA: 0s - loss: 0.0251 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0247 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0243 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - 3s 126us/step - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0139 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01763 to 0.01392, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 3/20\n",
      "19968/19968 [==============================] - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 2s - loss: 0.0230 - acc: 0.994 - ETA: 2s - loss: 0.0336 - acc: 0.991 - ETA: 2s - loss: 0.0267 - acc: 0.992 - ETA: 2s - loss: 0.0310 - acc: 0.991 - ETA: 2s - loss: 0.0299 - acc: 0.991 - ETA: 2s - loss: 0.0282 - acc: 0.992 - ETA: 2s - loss: 0.0273 - acc: 0.992 - ETA: 2s - loss: 0.0303 - acc: 0.992 - ETA: 2s - loss: 0.0292 - acc: 0.992 - ETA: 2s - loss: 0.0276 - acc: 0.992 - ETA: 2s - loss: 0.0270 - acc: 0.992 - ETA: 2s - loss: 0.0265 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0257 - acc: 0.992 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0253 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0239 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0235 - acc: 0.992 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0229 - acc: 0.992 - ETA: 0s - loss: 0.0226 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0226 - acc: 0.992 - ETA: 0s - loss: 0.0227 - acc: 0.992 - 3s 134us/step - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0125 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01392 to 0.01245, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 4/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0197 - acc: 0.992 - ETA: 2s - loss: 0.0123 - acc: 0.998 - ETA: 2s - loss: 0.0331 - acc: 0.992 - ETA: 2s - loss: 0.0317 - acc: 0.991 - ETA: 2s - loss: 0.0315 - acc: 0.992 - ETA: 2s - loss: 0.0268 - acc: 0.993 - ETA: 2s - loss: 0.0284 - acc: 0.992 - ETA: 2s - loss: 0.0290 - acc: 0.992 - ETA: 1s - loss: 0.0269 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.992 - ETA: 1s - loss: 0.0239 - acc: 0.993 - ETA: 1s - loss: 0.0225 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0224 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0213 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0217 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0205 - acc: 0.993 - 3s 136us/step - loss: 0.0207 - acc: 0.9937 - val_loss: 0.0125 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 3s - loss: 0.0077 - acc: 1.000 - ETA: 3s - loss: 0.0198 - acc: 0.996 - ETA: 3s - loss: 0.0189 - acc: 0.995 - ETA: 2s - loss: 0.0239 - acc: 0.992 - ETA: 2s - loss: 0.0228 - acc: 0.993 - ETA: 2s - loss: 0.0243 - acc: 0.992 - ETA: 2s - loss: 0.0232 - acc: 0.992 - ETA: 2s - loss: 0.0250 - acc: 0.992 - ETA: 2s - loss: 0.0238 - acc: 0.993 - ETA: 2s - loss: 0.0226 - acc: 0.993 - ETA: 2s - loss: 0.0236 - acc: 0.992 - ETA: 2s - loss: 0.0251 - acc: 0.992 - ETA: 2s - loss: 0.0236 - acc: 0.993 - ETA: 2s - loss: 0.0228 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.993 - ETA: 1s - loss: 0.0206 - acc: 0.993 - ETA: 1s - loss: 0.0201 - acc: 0.993 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0185 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0191 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0195 - acc: 0.993 - ETA: 0s - loss: 0.0196 - acc: 0.993 - ETA: 0s - loss: 0.0193 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.993 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - 3s 140us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0116 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01245 to 0.01160, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 6/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0222 - acc: 0.992 - ETA: 2s - loss: 0.0202 - acc: 0.993 - ETA: 2s - loss: 0.0213 - acc: 0.993 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0196 - acc: 0.994 - ETA: 2s - loss: 0.0218 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 1s - loss: 0.0195 - acc: 0.993 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0165 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0171 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0182 - acc: 0.994 - 2s 123us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0120 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0170 - acc: 0.993 - ETA: 2s - loss: 0.0192 - acc: 0.992 - ETA: 2s - loss: 0.0175 - acc: 0.992 - ETA: 1s - loss: 0.0193 - acc: 0.992 - ETA: 1s - loss: 0.0166 - acc: 0.993 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0142 - acc: 0.994 - ETA: 1s - loss: 0.0150 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.993 - ETA: 1s - loss: 0.0190 - acc: 0.993 - ETA: 1s - loss: 0.0193 - acc: 0.993 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 1s - loss: 0.0184 - acc: 0.993 - ETA: 1s - loss: 0.0186 - acc: 0.993 - ETA: 0s - loss: 0.0180 - acc: 0.994 - ETA: 0s - loss: 0.0184 - acc: 0.993 - ETA: 0s - loss: 0.0179 - acc: 0.993 - ETA: 0s - loss: 0.0176 - acc: 0.993 - ETA: 0s - loss: 0.0176 - acc: 0.993 - ETA: 0s - loss: 0.0181 - acc: 0.993 - ETA: 0s - loss: 0.0182 - acc: 0.993 - ETA: 0s - loss: 0.0178 - acc: 0.993 - ETA: 0s - loss: 0.0177 - acc: 0.993 - ETA: 0s - loss: 0.0173 - acc: 0.993 - ETA: 0s - loss: 0.0170 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0172 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - 2s 116us/step - loss: 0.0175 - acc: 0.9941 - val_loss: 0.0125 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0080 - acc: 1.000 - ETA: 2s - loss: 0.0097 - acc: 0.995 - ETA: 2s - loss: 0.0125 - acc: 0.994 - ETA: 1s - loss: 0.0105 - acc: 0.995 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0095 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.995 - ETA: 1s - loss: 0.0106 - acc: 0.995 - ETA: 1s - loss: 0.0104 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0124 - acc: 0.995 - ETA: 1s - loss: 0.0123 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0137 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - 2s 109us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0129 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0240 - acc: 0.984 - ETA: 2s - loss: 0.0437 - acc: 0.992 - ETA: 2s - loss: 0.0323 - acc: 0.992 - ETA: 1s - loss: 0.0235 - acc: 0.994 - ETA: 1s - loss: 0.0262 - acc: 0.994 - ETA: 1s - loss: 0.0237 - acc: 0.994 - ETA: 1s - loss: 0.0232 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.995 - ETA: 1s - loss: 0.0191 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.994 - ETA: 0s - loss: 0.0167 - acc: 0.994 - 2s 110us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0115 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01160 to 0.01155, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 10/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0158 - acc: 1.000 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0230 - acc: 0.994 - ETA: 2s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0155 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.995 - 2s 110us/step - loss: 0.0161 - acc: 0.9957 - val_loss: 0.0131 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0178 - acc: 0.992 - ETA: 2s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0136 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0124 - acc: 0.995 - ETA: 1s - loss: 0.0118 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - 2s 114us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01155 to 0.01143, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 12/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0146 - acc: 0.992 - ETA: 1s - loss: 0.0104 - acc: 0.993 - ETA: 1s - loss: 0.0081 - acc: 0.995 - ETA: 1s - loss: 0.0098 - acc: 0.996 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0193 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.995 - ETA: 0s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - 2s 111us/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01143 to 0.01139, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0107 - acc: 0.992 - ETA: 2s - loss: 0.0076 - acc: 0.996 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.995 - ETA: 1s - loss: 0.0190 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.993 - ETA: 1s - loss: 0.0210 - acc: 0.993 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0201 - acc: 0.993 - ETA: 1s - loss: 0.0191 - acc: 0.993 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0160 - acc: 0.994 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0156 - acc: 0.994 - ETA: 0s - loss: 0.0154 - acc: 0.994 - ETA: 0s - loss: 0.0151 - acc: 0.994 - ETA: 0s - loss: 0.0149 - acc: 0.994 - ETA: 0s - loss: 0.0149 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0161 - acc: 0.994 - 2s 113us/step - loss: 0.0159 - acc: 0.9946 - val_loss: 0.0117 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0142 - acc: 0.997 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 1s - loss: 0.0153 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - 2s 120us/step - loss: 0.0156 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0086 - acc: 0.998 - ETA: 1s - loss: 0.0149 - acc: 0.997 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - 2s 113us/step - loss: 0.0149 - acc: 0.9957 - val_loss: 0.0132 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0213 - acc: 0.992 - ETA: 2s - loss: 0.0072 - acc: 0.996 - ETA: 2s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0124 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0127 - acc: 0.996 - ETA: 1s - loss: 0.0124 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - 2s 112us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0119 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/19968 [==============================] - ETA: 2s - loss: 7.6557e-04 - acc: 1.000 - ETA: 2s - loss: 0.0232 - acc: 0.9922    - ETA: 2s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0136 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0189 - acc: 0.993 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0156 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0133 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0159 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - 2s 116us/step - loss: 0.0155 - acc: 0.9955 - val_loss: 0.0113 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01139 to 0.01131, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 18/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 8.9690e-04 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.0000    - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.997 - ETA: 0s - loss: 0.0118 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - 2s 112us/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.0112 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01131 to 0.01125, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 19/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0148 - acc: 0.992 - ETA: 2s - loss: 0.0153 - acc: 0.992 - ETA: 2s - loss: 0.0202 - acc: 0.992 - ETA: 2s - loss: 0.0208 - acc: 0.991 - ETA: 1s - loss: 0.0175 - acc: 0.993 - ETA: 1s - loss: 0.0183 - acc: 0.993 - ETA: 1s - loss: 0.0171 - acc: 0.993 - ETA: 1s - loss: 0.0178 - acc: 0.993 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 1s - loss: 0.0178 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0156 - acc: 0.994 - ETA: 1s - loss: 0.0153 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0151 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0153 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - 2s 119us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0110 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01125 to 0.01099, saving model to model_weights/adadelta_0.8.h5\n",
      "Epoch 20/20\n",
      "19968/19968 [==============================] - ETA: 2s - loss: 0.0056 - acc: 1.000 - ETA: 2s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0127 - acc: 0.995 - ETA: 1s - loss: 0.0130 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0101 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0109 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0124 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.996 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0128 - acc: 0.996 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.995 - 2s 114us/step - loss: 0.0142 - acc: 0.9958 - val_loss: 0.0123 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "history = train_model(0.8, 'adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182c80a10b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182bcb5bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
